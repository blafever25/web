[
  {
    "objectID": "mid_proj.html",
    "href": "mid_proj.html",
    "title": "Blake’s STAT515 Mid-Project",
    "section": "",
    "text": "Note: In conjunction with Lina Saade (Mid Project Group #4)\nLink to Presentation\n\nOriginal Visualization Analysis\n\nClick here for Original\n\nThe original visualization comes from a Wall Street Journal article entitled “What We Know About America’s Billionaires: 1,135 and Counting”, found here [1]. In the depiction, the reader finds that the writers have individualized every billionaire in America into estimated net worth (circles) and gender (color) and plotted onto a linear vertical axis depicting age at the time of writing [1]. Several notable billionaires are shown by name, however, most of the circles are unlabeled.\nStrengths of this visualization:\n\nAble to show all 1,135 billionaires with respective wealth, spread out and easy to view (when zoomed in).\nEasy relative comparison between circles\nAble to make quick generalizations about number, gender, and relative wealth across age ranges\n\nWeaknesses of this visualization:\n\nThere is a lot going on in the depiction making it difficult to focus on one thing.\nAnonymous information makes it difficult to compare two specific circles without additional context.\nCircle size is seemingly logarithmic making it difficult to be specific about wealth of any one circle.\nThe more circles in an age range, the more difficult to make any sort of comparison.\n\n\n\nSpecial Efforts\nI used ChatGPT 5 in order to summarize the count of circles, relative net worth of each circle, and the average net worth in each age range grouped by gender [2]. Initially, the counts the tool returned were clearly off, therefore, I had to prompt it with estimated numbers gained from visually counting the number of circles in each section. Additionally, the gender counts were off, so I added that to the prompt. Lastly, the article states that the collective wealth of these billionaires is roughly $5.7 trillion [1], so I added that to the prompt to ensure the math adds up in the end.\n\n\nRedesigned Visualization #1\n\n\n\n\n\n\nFor the first redesign, I decided to focus on totals for all billionaires and analyze the differences across age ranges. Additionally, as an additional layer, I color coded the age ranges based on combined total net worth. I used the generated data set from [2] and used [3] and [4] in conjunction with packages [5]-[8] to create both of these redesigns. From this plot we can make some general observations:\n\nThe number of billionaires in their 60s is almost double the number in their 50s.\nThe combined total net worth of the billionaires in their 70s is more than those in their 60s despite the total number being lower.\n\nIf you compare this graph with the original visualization, these points would be more difficult to make as the number of circles becomes large. Additionally, the total net worth numbers are nearly impossible to discern from the original. I used ChatGPT [2] to visually count and interpret the total net worth from the legend given (i.e. size of the circle correlates to estimated net worth), there could be some error introduced which could skew the numbers. However, a visual analysis of the original would take a reader a long time to compute a number for total net worth in high-circle age ranges, therefore the potential error introduced with the numbers by ChatGPT is acceptable.\nNext steps for this might be to gather more exact data, perhaps attempting to more accurately quantify the net worth for individual circles. Additionally, there may be data that can help add context. This may add another dimension to the visualization. Lastly, there may be a better way to display this information. Perhaps a box diagram that could display quartiles or summary statistics in a better way.\n\n\nRedesigned Visualization #2\n\n\n\n\n\n\nFor the second redesign, I decided to focus on the the differences in gender, both in count and in average net worth per person. For this, I asked ChatGPT [2] to separate the count and net worth based on gender and then calculate the average net worth for both genders for each age range. I then used [3] and [4] in conjunction with packages [5]-[8], to create a 100% stacked plot showing the average net worth percentages in each age range. From this plot, we can make the following general observations:\n\nThe average net worth across males is higher in every age range, indicating that gender disparity may exist even at the billionaire level.\nAs age range goes up, average female net worth goes up while male net worth goes down. This could indicate older female billionaires either do better with investing or hold on to their wealth better than their male counterparts.\n\nComparing this visualization to the original, it would be impossible to discern the average net worth across all circles in a given age range where the number of circles is large. While there may be some skewing of numbers due to use of ChatGPT [2] and as discussed in Re-design #1, it is likely minimal. Additionally, whereas the gender disparity issue is more visually noticeable in this redesign, it is also fairly easy to discern in the original visualization if the reader is specifically looking for it. In age ranges with larger circles, however, it becomes more difficult in the original to definitively identify a gap between females and males.\nNext steps for this visualization might include being able to add the percentage of women in each category. The proportion that I chose to display here is that of net wealth relative to men, however, the proportion of women to men is also interesting given the original diagram. Some analysis of that may benefit going forward.\n\n\nReferences\n[1] “What We Know About America’s Billionaires: 1,135 and Counting.” Accessed: Oct. 19, 2025. [Online]. Available: https://www.wsj.com/finance/investing/what-we-know-about-americas-billionaires-1-135-and-counting-98d22268\n[2] “Conversation with ChatGPT (GPT-5) about billionaire wealth distribution by age and gender.” Accessed: Oct. 18, 2025. [Online]. Available: https://chat.openai.com/\n[3] R Core Team, R: A Language and Environment for Statistical Computing, version 4.5.1, R Foundation for Statistical Computing, Vienna, Austria, 2025. [Online]. Available: https://www.R-project.org/\n[4] Posit Software, PBC, RStudio: Integrated Development Environment for R, version 2024.09.1, Boston, MA, USA, 2024. [Online]. Available: https://posit.co/products/open-source/rstudio/\n[5] H. Wickham, ggplot2: Elegant Graphics for Data Analysis, version 4.0.0, R package, 2025. [Online]. Available: https://CRAN.R-project.org/package=ggplot2\n[6] H. Wickham, R. François, L. Henry, K. Müller, D. Vaughan, dplyr: A Grammar of Data Manipulation, version 1.1.4, R package, 2023. [Online]. Available: https://CRAN.R-project.org/package=dplyr\n[7] H. Wickham, J. Grolemund, tidyverse: Easily Install and Load the ‘Tidyverse’, version 2.0.0, R package, 2023. [Online]. Available: https://CRAN.R-project.org/package=tidyverse\n[8] C. Sievert, plotly: Create Interactive Web Graphics via ‘plotly.js’, version 4.11.0, R package, 2025. [Online]. Available: https://CRAN.R-project.org/package=plotly\n\n\nCodes Used"
  },
  {
    "objectID": "final_proj_codes.html",
    "href": "final_proj_codes.html",
    "title": "A Multimodel Analysis of Income, Physical Activity, and Body Composition in U.S. Adults - CODES",
    "section": "",
    "text": "The following code was used in the Data Description section.\n\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(glmnet)\nlibrary(randomForest)\nlibrary(janitor)\nlibrary(psych)\nlibrary(plotly)\nlibrary(knitr)\nlibrary(kableExtra)\n\ndemo &lt;- read_xpt(\"../Final-Proj/DEMO_J.XPT\") %&gt;% clean_names()\nbmx  &lt;- read_xpt(\"../Final-Proj/BMX_J.XPT\")  %&gt;% clean_names()\npaq  &lt;- read_xpt(\"../Final-Proj/PAQ_J.XPT\")  %&gt;% clean_names()\n\ndemo &lt;- demo[demo$dmdeduc2 %in% c(1,2,3,4,5), ]\nnhanes &lt;- demo %&gt;%\n  select(\n    seqn,\n    ridageyr,     # age (years)\n    riagendr,     # sex\n    ridreth3,     # race/ethnicity\n    indfmpir,     # poverty income ratio\n    dmdeduc2      # education (adult)\n  ) %&gt;%\n  left_join(bmx %&gt;% select(seqn, bmxbmi), by = \"seqn\") %&gt;%\n  left_join(paq, by = \"seqn\") %&gt;%\n  mutate(\n    sex = factor(riagendr, levels = c(1,2), labels = c(\"Male\",\"Female\")),\n    race = factor(ridreth3),\n    education = factor(dmdeduc2),\n    age = ridageyr,\n    bmi = bmxbmi\n  )\n\n# ---------------------------\n# Derive MVPA minutes/week\n# ---------------------------\nnhanes &lt;- nhanes %&gt;%\n  mutate(\n    # Work vigorous\n    vig_work_minwk = if_else(paq605 == 1 & !is.na(paq610) & !is.na(pad615),\n                             as.numeric(paq610) * as.numeric(pad615), 0),\n    # Work moderate\n    mod_work_minwk = if_else(paq620 == 1 & !is.na(paq625) & !is.na(pad630),\n                             as.numeric(paq625) * as.numeric(pad630), 0),\n    # Rec vigorous\n    vig_rec_minwk  = if_else(paq650 == 1 & !is.na(paq655) & !is.na(pad660),\n                             as.numeric(paq655) * as.numeric(pad660), 0),\n    # Rec moderate\n    mod_rec_minwk  = if_else(paq665 == 1 & !is.na(paq670) & !is.na(pad675),\n                             as.numeric(paq670) * as.numeric(pad675), 0),\n    mvpa_minwk = vig_work_minwk + mod_work_minwk + vig_rec_minwk + mod_rec_minwk,\n    mvpa_equiv_minwk = (2 * (vig_work_minwk + vig_rec_minwk)) + \n      (mod_work_minwk + mod_rec_minwk)\n  )"
  },
  {
    "objectID": "final_proj_codes.html#data-description",
    "href": "final_proj_codes.html#data-description",
    "title": "A Multimodel Analysis of Income, Physical Activity, and Body Composition in U.S. Adults - CODES",
    "section": "",
    "text": "The following code was used in the Data Description section.\n\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(glmnet)\nlibrary(randomForest)\nlibrary(janitor)\nlibrary(psych)\nlibrary(plotly)\nlibrary(knitr)\nlibrary(kableExtra)\n\ndemo &lt;- read_xpt(\"../Final-Proj/DEMO_J.XPT\") %&gt;% clean_names()\nbmx  &lt;- read_xpt(\"../Final-Proj/BMX_J.XPT\")  %&gt;% clean_names()\npaq  &lt;- read_xpt(\"../Final-Proj/PAQ_J.XPT\")  %&gt;% clean_names()\n\ndemo &lt;- demo[demo$dmdeduc2 %in% c(1,2,3,4,5), ]\nnhanes &lt;- demo %&gt;%\n  select(\n    seqn,\n    ridageyr,     # age (years)\n    riagendr,     # sex\n    ridreth3,     # race/ethnicity\n    indfmpir,     # poverty income ratio\n    dmdeduc2      # education (adult)\n  ) %&gt;%\n  left_join(bmx %&gt;% select(seqn, bmxbmi), by = \"seqn\") %&gt;%\n  left_join(paq, by = \"seqn\") %&gt;%\n  mutate(\n    sex = factor(riagendr, levels = c(1,2), labels = c(\"Male\",\"Female\")),\n    race = factor(ridreth3),\n    education = factor(dmdeduc2),\n    age = ridageyr,\n    bmi = bmxbmi\n  )\n\n# ---------------------------\n# Derive MVPA minutes/week\n# ---------------------------\nnhanes &lt;- nhanes %&gt;%\n  mutate(\n    # Work vigorous\n    vig_work_minwk = if_else(paq605 == 1 & !is.na(paq610) & !is.na(pad615),\n                             as.numeric(paq610) * as.numeric(pad615), 0),\n    # Work moderate\n    mod_work_minwk = if_else(paq620 == 1 & !is.na(paq625) & !is.na(pad630),\n                             as.numeric(paq625) * as.numeric(pad630), 0),\n    # Rec vigorous\n    vig_rec_minwk  = if_else(paq650 == 1 & !is.na(paq655) & !is.na(pad660),\n                             as.numeric(paq655) * as.numeric(pad660), 0),\n    # Rec moderate\n    mod_rec_minwk  = if_else(paq665 == 1 & !is.na(paq670) & !is.na(pad675),\n                             as.numeric(paq670) * as.numeric(pad675), 0),\n    mvpa_minwk = vig_work_minwk + mod_work_minwk + vig_rec_minwk + mod_rec_minwk,\n    mvpa_equiv_minwk = (2 * (vig_work_minwk + vig_rec_minwk)) + \n      (mod_work_minwk + mod_rec_minwk)\n  )"
  },
  {
    "objectID": "final_proj_codes.html#rq-analysis",
    "href": "final_proj_codes.html#rq-analysis",
    "title": "A Multimodel Analysis of Income, Physical Activity, and Body Composition in U.S. Adults - CODES",
    "section": "2 RQ Analysis",
    "text": "2 RQ Analysis\n\n2.1 RQ1\nThe following code was used in Research Question 1 analysis.\n\n#imports\nlibrary(caret)\nlibrary(haven)\nlibrary(glmnet)\nlibrary(randomForest)\n\n# Read in source\nhealth = read.csv(\"../final-proj/final_project.csv\")\n\n# Data cleanup\nhealth$RIAGENDR = as.factor(health$RIAGENDR)\nhealth$DMDEDUC2 = as.factor(health$DMDEDUC2)\nhealth$DMDEDUC3 = as.factor(health$DMDEDUC3)\nhealth$INDHHIN2 = as.factor(health$INDHHIN2)\nhealth$INDFMIN2 = as.factor(health$INDFMIN2)\n\n# How well do daily sitting time and age predict BMI?\nset.seed(1)\nbmi = lm(BMXBMI~PAD680 + RIDAGEYR, data=health)\nsummary(bmi)\n\n\nCall:\nlm(formula = BMXBMI ~ PAD680 + RIDAGEYR, data = health)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-17.896  -5.110  -1.232   3.761  56.783 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 2.900e+01  2.917e-01  99.399   &lt;2e-16 ***\nPAD680      3.291e-04  1.358e-04   2.423   0.0154 *  \nRIDAGEYR    1.144e-02  5.443e-03   2.102   0.0356 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.436 on 5422 degrees of freedom\n  (3829 observations deleted due to missingness)\nMultiple R-squared:  0.001962,  Adjusted R-squared:  0.001594 \nF-statistic: 5.329 on 2 and 5422 DF,  p-value: 0.004872\n\n# Adding in gender\nbmi2 = lm(BMXBMI~PAD680 + RIDAGEYR + RIAGENDR, data=health)\nsummary(bmi2)\n\n\nCall:\nlm(formula = BMXBMI ~ PAD680 + RIDAGEYR + RIAGENDR, data = health)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-17.329  -5.089  -1.261   3.811  57.252 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 2.851e+01  3.110e-01  91.689  &lt; 2e-16 ***\nPAD680      3.178e-04  1.356e-04   2.344   0.0191 *  \nRIDAGEYR    1.194e-02  5.435e-03   2.197   0.0281 *  \nRIAGENDR2   8.982e-01  2.018e-01   4.451  8.7e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.424 on 5421 degrees of freedom\n  (3829 observations deleted due to missingness)\nMultiple R-squared:  0.005597,  Adjusted R-squared:  0.005047 \nF-statistic: 10.17 on 3 and 5421 DF,  p-value: 1.121e-06\n\n# Adding in interaction term\nbmi3 = lm(BMXBMI~RIDAGEYR + RIAGENDR + PAD680 + RIAGENDR:PAD680, data=health)\nsummary(bmi3)\n\n\nCall:\nlm(formula = BMXBMI ~ RIDAGEYR + RIAGENDR + PAD680 + RIAGENDR:PAD680, \n    data = health)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.170  -5.097  -1.254   3.800  57.180 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      2.860e+01  3.181e-01  89.900  &lt; 2e-16 ***\nRIDAGEYR         1.199e-02  5.434e-03   2.207 0.027375 *  \nRIAGENDR2        7.626e-01  2.289e-01   3.331 0.000872 ***\nPAD680           8.370e-05  2.308e-04   0.363 0.716853    \nRIAGENDR2:PAD680 3.573e-04  2.850e-04   1.253 0.210082    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.423 on 5420 degrees of freedom\n  (3829 observations deleted due to missingness)\nMultiple R-squared:  0.005885,  Adjusted R-squared:  0.005151 \nF-statistic: 8.021 on 4 and 5420 DF,  p-value: 1.916e-06\n\n# Cross-validation\ndata1 = subset(health, select = c(BMXBMI, PAD680, RIAGENDR, RIDAGEYR))\ndata1 = na.omit(data1)\n\n\n# Specify cross-validation method\ntrain_control = trainControl(method=\"CV\", number = 10)\n\n#fit a regression model and use k-fold CV to evaluate performance\nmodel1 = train(BMXBMI~PAD680 + RIDAGEYR + RIAGENDR + RIAGENDR:PAD680, \n               data =data1, method =\"lm\", \n               trControl = train_control)\n\nprint(model1)\n\nLinear Regression \n\n5425 samples\n   3 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 4883, 4882, 4883, 4882, 4882, 4883, ... \nResampling results:\n\n  RMSE     Rsquared     MAE     \n  7.42033  0.005270328  5.626383\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\n\n\n2.2 RQ2\n\n2.2.1 Setup (a)\nThe following code was used in setup and exploration of Research Question 2, part (a).\n\n# ---------------------------\n# Start RQ2, part (a)\n# Clean dataset/explore\n# ---------------------------\nnhanes &lt;- nhanes[nhanes$mvpa_equiv_minwk&lt;19000, ]\nrq2a_data &lt;- nhanes %&gt;%\n  transmute(\n    bmi,\n    age,\n    sex,\n    race,\n    indfmpir,\n    education,\n    mvpa_equiv_minwk\n  ) %&gt;%\n  na.omit()\n\nkable(summary(rq2a_data), caption = \"Summary Statistics\", align = \"c\")\n\n\nSummary Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\nbmi\nage\nsex\nrace\nindfmpir\neducation\nmvpa_equiv_minwk\n\n\n\n\n\nMin. :14.20\nMin. :20.00\nMale :2162\n1: 562\nMin. :0.000\n1: 332\nMin. : 0.0\n\n\n\n1st Qu.:24.70\n1st Qu.:36.00\nFemale:2318\n2: 389\n1st Qu.:1.200\n2: 494\n1st Qu.: 0.0\n\n\n\nMedian :28.60\nMedian :52.00\nNA\n3:1635\nMedian :2.150\n3:1081\nMedian : 240.0\n\n\n\nMean :29.83\nMean :51.15\nNA\n4:1017\nMean :2.557\n4:1476\nMean : 985.8\n\n\n\n3rd Qu.:33.60\n3rd Qu.:65.00\nNA\n6: 645\n3rd Qu.:4.080\n5:1097\n3rd Qu.: 1080.0\n\n\n\nMax. :86.20\nMax. :80.00\nNA\n7: 232\nMax. :5.000\nNA\nMax. :14580.0\n\n\n\n\npairs.panels(rq2a_data)\n\n\n\n\n\n\n\nbmi &lt;- ggplot(rq2a_data, aes(x = bmi)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 35) +\n  geom_density(linewidth = 1) +\n  labs(\n    title = \"Distribution of Body Mass Index (BMI)\",\n    x = \"BMI (kg/m²)\",\n    y = \"Count\"\n  ) + \n  xlim(10, 65)\nggplotly(bmi)\n\n\n\n\nggplot(rq2a_data, aes(x = indfmpir, y = bmi)) +\n  geom_point(alpha = 0.3) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  labs(\n    title = \"Relationship Between Poverty-Income Ratio and BMI\",\n    x = \"Poverty-Income Ratio (INDFMPIR)\",\n    y = \"BMI (kg/m²)\"\n  )\n\n\n\n\n\n\n\n\n\n\n2.2.2 MLR (a)\nThe following code was used in Research Question 2, part (a) MLR analysis.\n\n# ---------------------------\n# Train/test split\n# ---------------------------\nset.seed(25)\nn &lt;- nrow(rq2a_data)\nidx &lt;- sample.int(n, size = floor(0.80 * n), replace = FALSE)\n\ntrain &lt;- rq2a_data[idx, ]\ntest  &lt;- rq2a_data[-idx, ]\n\n# Helper metrics functions\nrmse &lt;- function(y, yhat) sqrt(mean((y - yhat)^2))\nmae  &lt;- function(y, yhat) mean(abs(y - yhat))\nrsq  &lt;- function(y, yhat) 1 - sum((y - yhat)^2) / sum((y - mean(y))^2)\n\n# ---------------------------\n# Multiple Linear Regression (MLR) + manual K-fold CV\n# ---------------------------\nK &lt;- 5\nfold_id &lt;- sample(rep(1:K, length.out = nrow(train)))\n\ncv_metrics_lm &lt;- data.frame(fold = 1:K, rmse = NA, mae = NA, rsq = NA)\n\nfor (k in 1:K) {\n  tr_k &lt;- train[fold_id != k, ]\n  va_k &lt;- train[fold_id == k, ]\n  \n  fit_lm &lt;- lm(bmi ~ indfmpir + age + sex + race + education + \n                 mvpa_equiv_minwk, data = tr_k)\n  pred   &lt;- predict(fit_lm, newdata = va_k)\n  \n  cv_metrics_lm$rmse[k] &lt;- rmse(va_k$bmi, pred)\n  cv_metrics_lm$mae[k]  &lt;- mae(va_k$bmi, pred)\n  cv_metrics_lm$rsq[k]  &lt;- rsq(va_k$bmi, pred)\n}\n\nkable(\n  cv_metrics_lm %&gt;%\n    summarize(\n      rmse_mean = mean(rmse), rmse_sd = sd(rmse),\n      mae_mean  = mean(mae),  mae_sd  = sd(mae),\n      rsq_mean  = mean(rsq),  rsq_sd  = sd(rsq)\n    ),\n  caption = \"RQ2a - MLR CV (mean +/- sd)\",\n  align = \"c\"\n)\n\n\nRQ2a - MLR CV (mean +/- sd)\n\n\nrmse_mean\nrmse_sd\nmae_mean\nmae_sd\nrsq_mean\nrsq_sd\n\n\n\n\n7.234845\n0.2995047\n5.394304\n0.2391558\n0.055296\n0.0110509\n\n\n\n\nfinal_lm &lt;- lm(bmi ~ indfmpir + age + sex + race + education + \n                 mvpa_equiv_minwk, data = train)\npred_lm  &lt;- predict(final_lm, newdata = test)\n\nkable(data.frame(\n  rmse = rmse(test$bmi, pred_lm),\n  mae  = mae(test$bmi, pred_lm),\n  rsq  = rsq(test$bmi, pred_lm)\n),\ncaption = \"RQ2a - Test metrics (MLR)\",\nalign = \"c\"\n)\n\n\nRQ2a - Test metrics (MLR)\n\n\nrmse\nmae\nrsq\n\n\n\n\n7.085617\n5.449196\n0.045798\n\n\n\n\nprint(summary(final_lm))\n\n\nCall:\nlm(formula = bmi ~ indfmpir + age + sex + race + education + \n    mvpa_equiv_minwk, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.376  -4.776  -0.864   3.599  54.275 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       3.170e+01  6.733e-01  47.074  &lt; 2e-16 ***\nindfmpir         -1.922e-04  8.740e-02  -0.002   0.9982    \nage              -1.409e-02  7.311e-03  -1.927   0.0541 .  \nsexFemale         5.451e-01  2.475e-01   2.202   0.0277 *  \nrace2            -1.281e+00  5.358e-01  -2.392   0.0168 *  \nrace3            -1.036e+00  4.173e-01  -2.482   0.0131 *  \nrace4             2.908e-01  4.433e-01   0.656   0.5119    \nrace6            -4.830e+00  4.969e-01  -9.720  &lt; 2e-16 ***\nrace7            -6.333e-01  6.423e-01  -0.986   0.3243    \neducation2       -7.049e-01  5.868e-01  -1.201   0.2297    \neducation3        2.945e-01  5.341e-01   0.552   0.5813    \neducation4        6.460e-01  5.321e-01   1.214   0.2248    \neducation5       -6.967e-01  5.769e-01  -1.208   0.2273    \nmvpa_equiv_minwk -3.255e-04  7.526e-05  -4.325 1.56e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.229 on 3570 degrees of freedom\nMultiple R-squared:  0.06296,   Adjusted R-squared:  0.05955 \nF-statistic: 18.45 on 13 and 3570 DF,  p-value: &lt; 2.2e-16\n\nincome_grid &lt;- seq(min(rq2a_data$indfmpir), max(rq2a_data$indfmpir), \n                   length.out = 100)\n\npred_df &lt;- data.frame(\n  indfmpir = income_grid,\n  age = mean(rq2a_data$age),\n  sex = levels(rq2a_data$sex)[1],\n  race = levels(rq2a_data$race)[1],\n  education = levels(rq2a_data$education)[1],\n  mvpa_equiv_minwk = mean(rq2a_data$mvpa_equiv_minwk)\n)\n\npred_df$pred_bmi &lt;- predict(final_lm, newdata = pred_df)\n\nggplot(pred_df, aes(x = indfmpir, y = pred_bmi)) +\n  geom_line(linewidth = 1) +\n  labs(\n    title = \"Association Between Income and BMI\",\n    subtitle = \"Predicted BMI holding demographics and physical activity \n    constant\",\n    x = \"Poverty-Income Ratio\",\n    y = \"Predicted BMI (kg/m²)\"\n  )\n\n\n\n\n\n\n\n\n\n\n2.2.3 Ridge/Lasso (a)\nThe following code was used in Research Question 2, part (a) Ridge/Lasso regression analysis.\n\n# ---------------------------\n# Ridge/Lasso\n# ---------------------------\nx_train &lt;- model.matrix(bmi ~  indfmpir + age + sex + race + education + \n                          mvpa_equiv_minwk, data = train)[, -1]\ny_train &lt;- train$bmi\n\nx_test  &lt;- model.matrix(bmi ~ indfmpir + age + sex + race + education + \n                          mvpa_equiv_minwk, data = test)[, -1]\ny_test  &lt;- test$bmi\n\nmu &lt;- colMeans(x_train)\nsdv &lt;- apply(x_train, 2, sd)\nsdv[sdv == 0] &lt;- 1\n\nx_train_sc &lt;- scale(x_train, center = mu, scale = sdv)\nx_test_sc  &lt;- scale(x_test,  center = mu, scale = sdv)\n\n# ---- Ridge (alpha = 0) ----\ncv_ridge &lt;- cv.glmnet(x_train_sc, y_train, alpha = 0, nfolds = 5)\n\npred_ridge &lt;- as.numeric(predict(cv_ridge, newx = x_test_sc, \n                                 s = \"lambda.min\"))\n\nkable(\n  data.frame(\n    rmse = rmse(y_test, pred_ridge),\n    mae  = mae(y_test, pred_ridge),\n    rsq  = rsq(y_test, pred_ridge)\n  ),\n  caption = \"RQ2a - Test metrics (Ridge)\",\n  align = \"c\"\n)\n\n\nRQ2a - Test metrics (Ridge)\n\n\nrmse\nmae\nrsq\n\n\n\n\n7.082975\n5.448042\n0.0465095\n\n\n\n\n# Ridge coefficients at best lambda\nridge_coefs &lt;- coef(cv_ridge, s = \"lambda.min\")\ncat(\"\\nRQ2 - Ridge non-zero coefficients:\\n\")\n\n\nRQ2 - Ridge non-zero coefficients:\n\nprint(ridge_coefs[ridge_coefs[,1] != 0, , drop = FALSE])\n\n14 x 1 sparse Matrix of class \"dgCMatrix\"\n                   lambda.min\n(Intercept)      29.838141741\nindfmpir         -0.009441769\nage              -0.238566121\nsexFemale         0.269232584\nrace2            -0.291696061\nrace3            -0.386278226\nrace4             0.208332433\nrace6            -1.551783508\nrace7            -0.090040940\neducation2       -0.223951034\neducation3        0.104602178\neducation4        0.275432485\neducation5       -0.325164272\nmvpa_equiv_minwk -0.528147121\n\n# ---- Lasso (alpha = 1) ----\ncv_lasso &lt;- cv.glmnet(x_train_sc, y_train, alpha = 1, nfolds = 5)\n\npred_lasso &lt;- as.numeric(predict(cv_lasso, newx = x_test_sc, \n                                 s = \"lambda.min\"))\n\nkable(\n  data.frame(\n    rmse = rmse(y_test, pred_lasso),\n    mae  = mae(y_test, pred_lasso),\n    rsq  = rsq(y_test, pred_lasso)\n    ),\n  caption = \"RQ2a - Test metrics (Lasso)\",\n  align = \"c\"\n)\n\n\nRQ2a - Test metrics (Lasso)\n\n\nrmse\nmae\nrsq\n\n\n\n\n7.076227\n5.444314\n0.0483253\n\n\n\n\nlasso_coefs &lt;- coef(cv_lasso, s = \"lambda.min\")\ncat(\"\\nRQ2 - Lasso non-zero coefficients:\\n\")\n\n\nRQ2 - Lasso non-zero coefficients:\n\nprint(lasso_coefs[lasso_coefs[,1] != 0, , drop = FALSE])\n\n13 x 1 sparse Matrix of class \"dgCMatrix\"\n                  lambda.min\n(Intercept)      29.83814174\nage              -0.20831541\nsexFemale         0.25037652\nrace2            -0.23218973\nrace3            -0.30376428\nrace4             0.24154263\nrace6            -1.52365740\nrace7            -0.02354867\neducation2       -0.23449868\neducation3        0.01391149\neducation4        0.18666544\neducation5       -0.36057169\nmvpa_equiv_minwk -0.49531305\n\ncoef_df &lt;- bind_rows(\n  data.frame(term = names(coef(final_lm)), estimate = coef(final_lm), \n             model = \"MLR\"),\n  data.frame(term = rownames(ridge_coefs), estimate = ridge_coefs[,1], \n             model = \"Ridge\"),\n  data.frame(term = rownames(lasso_coefs), estimate = lasso_coefs[,1], \n             model = \"Lasso\")\n) %&gt;%\n  filter(term != \"(Intercept)\", grepl(\"indfmpir\", term))\n\nggplot(coef_df, aes(x = model, y = estimate)) +\n  geom_col() +\n  labs(\n    title = \"Income Coefficient Across Modeling Techniques\",\n    y = \"Estimated Effect on BMI\",\n    x = NULL\n  )\n\n\n\n\n\n\n\n\n\n\n2.2.4 Setup (b)\nThe following code was used in setup and exploration of Research Question 2, part (b).\n\n# ---------------------------\n# Start RQ2, part (b)\n# Clean dataset\n# ---------------------------\nrq2b_data &lt;- nhanes %&gt;%\n  transmute(\n    obese = ifelse(bmi &gt;= 30, 1, 0),     # 1=obese, 0=not obese\n    indfmpir,\n    age,\n    sex,\n    race,\n    education,\n    mvpa_equiv_minwk\n  ) %&gt;%\n  na.omit()\n\nrq2b_data$obese &lt;- as.integer(rq2b_data$obese)\n\nrq2b_data %&gt;%\n  mutate(income_group = cut(indfmpir,\n                            breaks = c(0, 1, 2, 3, 4, Inf),\n                            labels = c(\"&lt;1\", \"1–2\", \"2–3\", \"3-4\", \"&gt;4\"))) %&gt;%\n  group_by(income_group) %&gt;%\n  summarize(obesity_rate = mean(obese)) %&gt;%\n  ggplot(aes(x = income_group, y = obesity_rate)) +\n  geom_col() +\n  labs(\n    title = \"Obesity Prevalence by Household Income Group\",\n    x = \"Poverty-Income Ratio Group\",\n    y = \"Proportion Obese\"\n  )\n\n\n\n\n\n\n\n\n\n\n2.2.5 Log Regression (b)\nThe following code was used in Research Question 2, part (b) Logistic Regression analysis.\n\n# ---------------------------\n# Train/test split\n# ---------------------------\nn &lt;- nrow(rq2b_data)\nidx &lt;- sample.int(n, size = floor(0.80 * n), replace = FALSE)\n\ntrain &lt;- rq2b_data[idx, ]\ntest  &lt;- rq2b_data[-idx, ]\n\n# ---------------------------\n# Metrics\n# ---------------------------\nmetrics_binary &lt;- function(truth01, prob, threshold = 0.5) {\n  pred01 &lt;- ifelse(prob &gt;= threshold, 1, 0)\n  \n  tp &lt;- sum(pred01 == 1 & truth01 == 1)\n  tn &lt;- sum(pred01 == 0 & truth01 == 0)\n  fp &lt;- sum(pred01 == 1 & truth01 == 0)\n  fn &lt;- sum(pred01 == 0 & truth01 == 1)\n  \n  acc  &lt;- (tp + tn) / (tp + tn + fp + fn)\n  sens &lt;- ifelse((tp + fn) == 0, NA, tp / (tp + fn))  # recall\n  spec &lt;- ifelse((tn + fp) == 0, NA, tn / (tn + fp))\n  prec &lt;- ifelse((tp + fp) == 0, NA, tp / (tp + fp))\n  f1   &lt;- ifelse(is.na(prec) | is.na(sens) | (prec + sens) == 0, NA, \n                 2 * prec * sens / (prec + sens))\n  bal_acc &lt;- mean(c(sens, spec), na.rm = TRUE)\n  \n  data.frame(\n    accuracy = acc,\n    sensitivity = sens,\n    specificity = spec,\n    precision = prec,\n    f1 = f1,\n    bal_accuracy = bal_acc,\n    tp = tp, tn = tn, fp = fp, fn = fn\n  )\n}\n\n# ---------------------------\n# Logistic regression + K-fold CV\n# ---------------------------\nK &lt;- 5\nfold_id &lt;- sample(rep(1:K, length.out = nrow(train)))\n\ncv_out &lt;- vector(\"list\", K)\n\nfor (k in 1:K) {\n  tr_k &lt;- train[fold_id != k, ]\n  va_k &lt;- train[fold_id == k, ]\n  \n  fit &lt;- glm(\n    obese ~ indfmpir + age + sex + race + education + mvpa_equiv_minwk,\n    data = tr_k,\n    family = binomial()\n  )\n  \n  p &lt;- predict(fit, newdata = va_k, type = \"response\")\n  \n  cv_out[[k]] &lt;- cbind(\n    fold = k,\n    metrics_binary(va_k$obese, p, threshold = 0.5)\n  )\n}\n\ncv_df &lt;- bind_rows(cv_out)\n\ncat(\"\\nRQ2b Logistic Reg - CV mean +/- sd:\\n\")\n\n\nRQ2b Logistic Reg - CV mean +/- sd:\n\nprint(\n  cv_df %&gt;%\n    summarize(\n      acc_mean = mean(accuracy), acc_sd = sd(accuracy),\n      sens_mean = mean(sensitivity, na.rm = TRUE), \n      sens_sd = sd(sensitivity, na.rm = TRUE),\n      spec_mean = mean(specificity, na.rm = TRUE), \n      spec_sd = sd(specificity, na.rm = TRUE),\n      prec_mean = mean(precision, na.rm = TRUE), \n      prec_sd = sd(precision, na.rm = TRUE),\n      f1_mean = mean(f1, na.rm = TRUE), f1_sd = sd(f1, na.rm = TRUE),\n      balacc_mean = mean(bal_accuracy, na.rm = TRUE), \n      balacc_sd = sd(bal_accuracy, na.rm = TRUE)\n    )\n)\n\n   acc_mean     acc_sd sens_mean    sens_sd spec_mean    spec_sd prec_mean\n1 0.5853903 0.02385022 0.3027356 0.03990417 0.7901275 0.03446358 0.5104574\n     prec_sd   f1_mean      f1_sd balacc_mean  balacc_sd\n1 0.04315099 0.3785182 0.03668177   0.5464316 0.02183762\n\n# ---------------------------\n# Fit final logistic model + test evaluation\n# ---------------------------\nfinal_fit &lt;- glm(\n  obese ~ indfmpir + age + sex + race + education + mvpa_equiv_minwk,\n  data = train,\n  family = binomial()\n)\n\np_test &lt;- predict(final_fit, newdata = test, type = \"response\")\n\ncat(\"\\nRQ2b Logistic Reg - Test metrics:\\n\")\n\n\nRQ2b Logistic Reg - Test metrics:\n\nprint(metrics_binary(test$obese, p_test, threshold = 0.5))\n\n   accuracy sensitivity specificity precision        f1 bal_accuracy  tp  tn\n1 0.5636161   0.2797927   0.7784314 0.4886878 0.3558484    0.5291121 108 397\n   fp  fn\n1 113 278\n\ncat(\"\\nRQ2b Logistic Reg coefficients (log-odds scale):\\n\")\n\n\nRQ2b Logistic Reg coefficients (log-odds scale):\n\nprint(summary(final_fit)$coefficients)\n\n                      Estimate   Std. Error     z value     Pr(&gt;|z|)\n(Intercept)      -2.056987e-01 1.940086e-01  -1.0602559 2.890282e-01\nindfmpir          4.644851e-02 2.512022e-02   1.8490490 6.445074e-02\nage              -2.383539e-03 2.105895e-03  -1.1318413 2.577012e-01\nsexFemale         1.632590e-01 7.172178e-02   2.2762816 2.282916e-02\nrace2            -2.719289e-01 1.509794e-01  -1.8010998 7.168716e-02\nrace3            -1.802389e-01 1.175013e-01  -1.5339313 1.250465e-01\nrace4             7.468837e-02 1.242104e-01   0.6013055 5.476365e-01\nrace6            -1.700797e+00 1.665425e-01 -10.2123860 1.745197e-24\nrace7            -6.340947e-02 1.794530e-01  -0.3533486 7.238271e-01\neducation2        6.649891e-02 1.711187e-01   0.3886127 6.975627e-01\neducation3        1.627197e-01 1.569203e-01   1.0369572 2.997558e-01\neducation4        2.728823e-01 1.558155e-01   1.7513175 7.989123e-02\neducation5       -1.901487e-01 1.721756e-01  -1.1043880 2.694249e-01\nmvpa_equiv_minwk -3.210368e-05 2.100299e-05  -1.5285286 1.263814e-01\n\nor &lt;- exp(coef(final_fit))\nci &lt;- exp(confint(final_fit))\n\nkable(\n  data.frame(term = names(or), OR = or, CI_low = ci[,1], CI_high = ci[,2]),\n  caption = \"RQ2b Odds Ratios (OR) and 95% CI\",\n  align = \"c\"\n)\n\n\nRQ2b Odds Ratios (OR) and 95% CI\n\n\n\n\n\n\n\n\n\n\nterm\nOR\nCI_low\nCI_high\n\n\n\n\n(Intercept)\n(Intercept)\n0.8140783\n0.5560027\n1.1899345\n\n\nindfmpir\nindfmpir\n1.0475441\n0.9972640\n1.1004866\n\n\nage\nage\n0.9976193\n0.9935077\n1.0017446\n\n\nsexFemale\nsexFemale\n1.1773415\n1.0230182\n1.3551852\n\n\nrace2\nrace2\n0.7619084\n0.5662040\n1.0236062\n\n\nrace3\nrace3\n0.8350707\n0.6631901\n1.0513760\n\n\nrace4\nrace4\n1.0775483\n0.8446640\n1.3747322\n\n\nrace6\nrace6\n0.1825380\n0.1310610\n0.2519134\n\n\nrace7\nrace7\n0.9385591\n0.6596995\n1.3338589\n\n\neducation2\neducation2\n1.0687598\n0.7647463\n1.4962556\n\n\neducation3\neducation3\n1.1767068\n0.8662632\n1.6031492\n\n\neducation4\neducation4\n1.3137456\n0.9693418\n1.7861759\n\n\neducation5\neducation5\n0.8268362\n0.5905851\n1.1602587\n\n\nmvpa_equiv_minwk\nmvpa_equiv_minwk\n0.9999679\n0.9999265\n1.0000089\n\n\n\n\npred_logit &lt;- pred_df\npred_logit$prob_obese &lt;- predict(final_fit, newdata = pred_logit, \n                                 type = \"response\")\n\nggplot(pred_logit, aes(x = indfmpir, y = prob_obese)) +\n  geom_line(linewidth = 1) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  labs(\n    title = \"Probability of Obesity by Household Income\",\n    subtitle = \"Predicted probabilities holding demographics and activity constant\",\n    x = \"Poverty-Income Ratio\",\n    y = \"Predicted Probability of Obesity\"\n  )\n\n\n\n\n\n\n\nor_df &lt;- data.frame(\n  term = rownames(summary(final_fit)$coefficients),\n  estimate = summary(final_fit)$coefficients[,1],\n  se = summary(final_fit)$coefficients[,2]\n) %&gt;%\n  filter(term != \"(Intercept)\") %&gt;%\n  mutate(\n    OR = exp(estimate),\n    lo = exp(estimate - 1.96 * se),\n    hi = exp(estimate + 1.96 * se)\n  )\n\nggplot(or_df, aes(x = OR, y = reorder(term, OR))) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = lo, xmax = hi), height = 0.2) +\n  geom_vline(xintercept = 1, linetype = \"dashed\") +\n  labs(\n    title = \"Odds Ratios for Obesity (Logistic Regression)\",\n    x = \"Odds Ratio (95% CI)\",\n    y = NULL\n  )\n\n\n\n\n\n\n\n\n\n\n\n2.3 RQ3\nThe following code was used in Research Question 3 analysis.\n\nhealth2 = subset(health, select = c(RIAGENDR, RIDAGEYR, RIDRETH3, DMQMILIZ,\n                                    DMDCITZN,DMDEDUC2, DMDMARTL, DMDBORN4, \n                                    INDHHIN2, INDFMIN2, INDFMPIR, PAD660,\n                                    PAQ655, PAD675, PAQ670))\n\nhealth2 = na.omit(health2)\n\n# Add calculated leisure-time column/cleanup\nhealth2$total_leisure_PA &lt;- with(health2,(PAD660 * PAQ655) + (PAD675 * PAQ670))\nhealth2 = subset(health2, select = -c(PAD660, PAQ655, PAD675, PAQ670))\nhealth2$DMDEDUC2 = as.factor(health2$DMDEDUC2)\nhealth2$RIAGENDR = as.factor(health2$RIAGENDR)\nhealth2$INDHHIN2 = as.factor(health2$INDHHIN2)\nhealth2$INDFMIN2 = as.factor(health2$INDFMIN2)\n\n# Pairs\npairs.panels(health2)\n\n\n\n\n\n\n\n# Data prep\ny = health2$total_leisure_PA\nx = model.matrix(total_leisure_PA ~ ., health2)[,-1]\n\ntrain=sample(1:nrow(x), nrow(x)/2)\ntest=(-train)\ny.test = y[test]\ngrid=10^seq(10,-2,length=100)\n\n# LASSO\nlasso.mod=glmnet(x[train,], y[train], alpha=1, lambda=grid)\n\ncv.lasso=cv.glmnet(x[train,],y[train],alpha=1)\nplot(cv.lasso)\n\n\n\n\n\n\n\n# Getting the MSE for the LASSO model\nbestlam=cv.lasso$lambda.min\nlasso.pred=predict(lasso.mod,s=bestlam ,newx=x[test,])\nsqrt(mean((lasso.pred-y.test)^2))\n\n[1] 425.9082\n\nout=glmnet(x,y,alpha=1,lambda=grid)\npredict(out,type=\"coefficients\",s=bestlam)[1:12,]\n\n (Intercept)    RIAGENDR2     RIDAGEYR     RIDRETH3     DMQMILIZ     DMDCITZN \n490.39291448 -22.06657262  -0.70797843   0.06584831   0.00000000   0.00000000 \n   DMDEDUC22    DMDEDUC23    DMDEDUC24    DMDEDUC25    DMDEDUC27    DMDEDUC29 \n105.17706751   0.00000000   0.00000000 -38.72220774   0.00000000   0.00000000 \n\n# Ridge Regression\nridge.mod=glmnet(x,y,alpha=0,lambda=grid)\ncv.ridge = cv.glmnet(x[train ,],y[train],alpha=0)\nplot(cv.ridge)\n\n\n\n\n\n\n\n# Getting the MSE for the Ridge Regression model\nbestlam=cv.ridge$lambda.min\nridge.pred=predict(ridge.mod,s=bestlam ,newx=x[test,])\nsqrt(mean((ridge.pred-y.test)^2))\n\n[1] 417.0286\n\n# Coefficients\nout=glmnet(x,y,alpha=0,lambda=grid)\npredict(out,type=\"coefficients\",s=bestlam)[1:12,]\n\n(Intercept)   RIAGENDR2    RIDAGEYR    RIDRETH3    DMQMILIZ    DMDCITZN \n500.2858184 -20.0262810  -0.6231719   3.0257839  -1.2562350  -3.3760809 \n  DMDEDUC22   DMDEDUC23   DMDEDUC24   DMDEDUC25   DMDEDUC27   DMDEDUC29 \n 62.8706004  16.6736615   4.3829060 -22.8309922   0.0000000   0.0000000 \n\n# Random Forests\nrf.health2=randomForest(total_leisure_PA~.,data=health2,subset=train,mtry=6,importance=TRUE)\nyhat.rf = predict(rf.health2,newdata=health2[-train,])\nmean((yhat.rf-y.test)^2)\n\n[1] 183267.7\n\nsqrt(mean((yhat.rf-y.test)^2))\n\n[1] 428.0978\n\n# Importance\nimportance(rf.health2)\n\n             %IncMSE IncNodePurity\nRIAGENDR -0.68545060     1227172.3\nRIDAGEYR  0.04963857     7450183.5\nRIDRETH3  1.78743634     3834674.0\nDMQMILIZ  0.85243169      258252.1\nDMDCITZN  2.17401275      941210.1\nDMDEDUC2  5.63626197     5426340.5\nDMDMARTL  0.55017530     3569002.6\nDMDBORN4  3.35225882     1115497.6\nINDHHIN2 -0.17057506     5776478.1\nINDFMIN2 -2.16243290     5493280.9\nINDFMPIR  1.81024071     7027641.2\n\n# RF visualization\nvarImpPlot(rf.health2)\n\n\n\n\n\n\n\n\n\n\n2.4 RQ4\n\n2.4.1 Setup\nThe following code was used in setup and exploration of Research Question 4.\n\nlibrary(dplyr)\nlibrary(glmnet)\nlibrary(randomForest)\n\ndemo &lt;- read_xpt(\"../Final-Proj/DEMO_J.XPT\") %&gt;% clean_names()\nbmx  &lt;- read_xpt(\"../Final-Proj/BMX_J.XPT\")  %&gt;% clean_names()\npaq  &lt;- read_xpt(\"../Final-Proj/PAQ_J.XPT\")  %&gt;% clean_names()\n\ndemo &lt;- demo[demo$dmdeduc2 %in% c(1,2,3,4,5), ]\n\n# ---------------------------\n# Merge + select variables\n# ---------------------------\nnhanes &lt;- demo %&gt;%\n  select(seqn, ridageyr, riagendr, ridreth3, indfmpir, dmdeduc2) %&gt;%\n  left_join(bmx %&gt;% select(seqn, bmxwaist), by = \"seqn\") %&gt;%\n  left_join(paq, by = \"seqn\") %&gt;%\n  mutate(\n    age = ridageyr,\n    sex = factor(riagendr, levels = c(1, 2), labels = c(\"Male\", \"Female\")),\n    race = factor(ridreth3),\n    education = factor(dmdeduc2),\n    waist = bmxwaist\n  )\n\n# ---------------------------\n# Derive MVPA (equivalent minutes/week)\n# ---------------------------\nnhanes &lt;- nhanes %&gt;%\n  mutate(\n    # Work vigorous\n    vig_work_minwk = if_else(paq605 == 1 & !is.na(paq610) & !is.na(pad615),\n                             as.numeric(paq610) * as.numeric(pad615), 0),\n    # Work moderate\n    mod_work_minwk = if_else(paq620 == 1 & !is.na(paq625) & !is.na(pad630),\n                             as.numeric(paq625) * as.numeric(pad630), 0),\n    # Rec vigorous\n    vig_rec_minwk  = if_else(paq650 == 1 & !is.na(paq655) & !is.na(pad660),\n                             as.numeric(paq655) * as.numeric(pad660), 0),\n    # Rec moderate\n    mod_rec_minwk  = if_else(paq665 == 1 & !is.na(paq670) & !is.na(pad675),\n                             as.numeric(paq670) * as.numeric(pad675), 0),\n    mvpa_minwk = vig_work_minwk + mod_work_minwk + vig_rec_minwk + mod_rec_minwk,\n    mvpa_equiv_minwk = (2 * (vig_work_minwk + vig_rec_minwk)) + \n      (mod_work_minwk + mod_rec_minwk)\n  )\nnhanes &lt;- nhanes[nhanes$mvpa_equiv_minwk&lt;19000, ]\n\n# ---------------------------\n# Transform/Explore data\n# ---------------------------\nrq4_data &lt;- nhanes %&gt;%\n  transmute(\n    waist,\n    age,\n    sex,\n    race,\n    education,\n    indfmpir,\n    mvpa_equiv_minwk\n  ) %&gt;%\n  na.omit()\n\nggplot(rq4_data, aes(x = waist)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 35) +\n  geom_density(linewidth = 1) +\n  labs(title = \"Waist Circumference: Histogram + Density\", \n       x = \"Waist (cm)\", y = \"Density\")\n\n\n\n\n\n\n\nggplot(rq4_data, aes(x = mvpa_equiv_minwk, y = waist)) +\n  geom_point(alpha = 0.25) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  labs(\n    title = \"Waist Circumference vs MVPA (Equivalent Minutes/Week)\",\n    x = \"MVPA equivalent minutes/week\",\n    y = \"Waist circumference (cm)\"\n  )\n\n\n\n\n\n\n\nggplot(rq4_data, aes(x = sex, y = waist)) +\n  geom_boxplot() +\n  labs(title = \"Waist Circumference by Sex\", x = NULL, y = \"Waist (cm)\")\n\n\n\n\n\n\n\nggplot(rq4_data, aes(x = race, y = waist)) +\n  geom_boxplot() +\n  coord_flip() +\n  labs(title = \"Waist Circumference by Race/Ethnicity Code (RIDRETH3)\", \n       x = \"Race/Ethnicity Code\", y = \"Waist (cm)\")\n\n\n\n\n\n\n\n# ---------------------------\n# Train/test split\n# ---------------------------\nn &lt;- nrow(rq4_data)\nidx &lt;- sample.int(n, size = floor(0.80 * n), replace = FALSE)\ntrain &lt;- rq4_data[idx, ]\ntest  &lt;- rq4_data[-idx, ]\n\n# ---------------------------\n# Helper regression metrics\n# ---------------------------\nrmse &lt;- function(y, yhat) sqrt(mean((y - yhat)^2))\nmae  &lt;- function(y, yhat) mean(abs(y - yhat))\nrsq  &lt;- function(y, yhat) 1 - sum((y - yhat)^2) / sum((y - mean(y))^2)\n\n# ---------------------------\n# Build consistent model matrix for MLR + Ridge + Lasso\n# (handles factors with dummy variables)\n# ---------------------------\nform &lt;- waist ~ age + sex + race + education + indfmpir + mvpa_equiv_minwk\n\nx_train &lt;- model.matrix(form, data = train)[, -1]\ny_train &lt;- train$waist\n\nx_test  &lt;- model.matrix(form, data = test)[, -1]\ny_test  &lt;- test$waist\n\nmu  &lt;- colMeans(x_train)\nsdv &lt;- apply(x_train, 2, sd)\nsdv[sdv == 0] &lt;- 1\n\nx_train_sc &lt;- scale(x_train, center = mu, scale = sdv)\nx_test_sc  &lt;- scale(x_test,  center = mu, scale = sdv)\n\n\n\n2.4.2 MLR\nThe following code was used in Research Question 4 MLR analysis.\n\n# ---------------------------\n# Multiple Linear Regression (MLR) + manual K-fold CV\n# ---------------------------\nK &lt;- 5\nfold_id &lt;- sample(rep(1:K, length.out = nrow(x_train_sc)))\n\ncv_mlr &lt;- data.frame(fold = 1:K, rmse = NA, mae = NA, rsq = NA)\n\nfor (k in 1:K) {\n  tr_idx &lt;- which(fold_id != k)\n  va_idx &lt;- which(fold_id == k)\n  \n  fit_k &lt;- lm(y_train[tr_idx] ~ x_train_sc[tr_idx, ] )\n  pred  &lt;- predict(fit_k, newdata = data.frame(x_train_sc[va_idx, , \n                                                          drop = FALSE]))\n  \n  cv_mlr$rmse[k] &lt;- rmse(y_train[va_idx], pred)\n  cv_mlr$mae[k]  &lt;- mae(y_train[va_idx], pred)\n  cv_mlr$rsq[k]  &lt;- rsq(y_train[va_idx], pred)\n}\n\nkable(\n  data.frame(\n    rmse_mean = mean(cv_mlr$rmse), rmse_sd = sd(cv_mlr$rmse),\n    mae_mean  = mean(cv_mlr$mae),  mae_sd  = sd(cv_mlr$mae),\n    rsq_mean  = mean(cv_mlr$rsq),  rsq_sd  = sd(cv_mlr$rsq)\n    ),\n  caption = \"RQ4 - MLR CV mean +/- sd\",\n  align = \"c\"\n)\n\n\nRQ4 - MLR CV mean +/- sd\n\n\nrmse_mean\nrmse_sd\nmae_mean\nmae_sd\nrsq_mean\nrsq_sd\n\n\n\n\n18.14753\n0.3643531\n14.40548\n0.3331936\n-3.454478\n0.0860984\n\n\n\n\nmlr_final &lt;- lm(y_train ~ x_train_sc)\nmlr_pred  &lt;- as.numeric(predict(mlr_final, newdata = data.frame(x_test_sc)))\n\nkable(\n  data.frame(\n    rmse = rmse(y_test, mlr_pred),\n    mae  = mae(y_test, mlr_pred),\n    rsq  = rsq(y_test, mlr_pred)\n    ),\n  caption = \"RQ4 - Test metrics (MLR)\",\n  align = \"c\"\n)\n\n\nRQ4 - Test metrics (MLR)\n\n\nrmse\nmae\nrsq\n\n\n\n\n17.51308\n14.10237\n-3.552695\n\n\n\n\nmlr_coef &lt;- coef(mlr_final)\nmlr_imp &lt;- data.frame(\n  feature = names(mlr_coef)[-1],\n  importance = abs(mlr_coef[-1]),\n  method = \"MLR_abs_std_coef\"\n) %&gt;% arrange(desc(importance))\n\n\n\n2.4.3 Ridge Regression\nThe following code was used in Research Question 4 Ridge regression analysis.\n\n# ---------------------------\n# Ridge regression\n# ---------------------------\ncv_ridge &lt;- cv.glmnet(x_train_sc, y_train, alpha = 0, nfolds = 5)\n\nridge_pred &lt;- as.numeric(predict(cv_ridge, newx = x_test_sc, \n                                 s = \"lambda.min\"))\n\nkable(\n  data.frame(\n    rmse = rmse(y_test, ridge_pred),\n    mae  = mae(y_test, ridge_pred),\n    rsq  = rsq(y_test, ridge_pred)\n  ),\n  caption = \"RQ4 - Test metrics (Ridge)\",\n  align = \"c\"\n)\n\n\nRQ4 - Test metrics (Ridge)\n\n\nrmse\nmae\nrsq\n\n\n\n\n15.74937\n12.35245\n0.0795298\n\n\n\n\nridge_coef &lt;- as.matrix(coef(cv_ridge, s = \"lambda.min\"))\nridge_imp &lt;- data.frame(\n  feature = rownames(ridge_coef)[-1],\n  importance = abs(ridge_coef[-1, 1]),\n  method = \"Ridge_abs_coef\"\n) %&gt;% arrange(desc(importance))\n\n\n\n2.4.4 Lasso Regression\nThe following code was used in Research Question 4 Lasso regression analysis.\n\n# ---------------------------\n# Lasso regression\n# ---------------------------\ncv_lasso &lt;- cv.glmnet(x_train_sc, y_train, alpha = 1, nfolds = 5)\n\nlasso_pred &lt;- as.numeric(predict(cv_lasso, newx = x_test_sc, \n                                 s = \"lambda.min\"))\n\nkable(\n  data.frame(\n    rmse = rmse(y_test, lasso_pred),\n    mae  = mae(y_test, lasso_pred),\n    rsq  = rsq(y_test, lasso_pred)\n  ),\n  caption = \"RQ4 - Test metrics (Lasso)\",\n  align = \"c\"\n)\n\n\nRQ4 - Test metrics (Lasso)\n\n\nrmse\nmae\nrsq\n\n\n\n\n15.74135\n12.35013\n0.0804672\n\n\n\n\nlasso_coef &lt;- as.matrix(coef(cv_lasso, s = \"lambda.min\"))\nlasso_imp &lt;- data.frame(\n  feature = rownames(lasso_coef)[-1],\n  importance = abs(lasso_coef[-1, 1]),\n  method = \"Lasso_abs_coef\"\n) %&gt;% arrange(desc(importance))\n\ncat(\"\\nRQ4 - Lasso non-zero coefficients:\\n\")\n\n\nRQ4 - Lasso non-zero coefficients:\n\nprint(lasso_coef[lasso_coef[,1] != 0, , drop = FALSE])\n\n                   lambda.min\n(Intercept)      100.99956446\nage                2.04125055\nsexFemale         -2.03419634\nrace2             -0.91681708\nrace4             -0.44035809\nrace6             -4.05520974\nrace7             -0.03494382\neducation3         0.74317584\neducation4         1.35596545\neducation5        -0.15309078\nindfmpir          -0.15264476\nmvpa_equiv_minwk  -0.84597741\n\n\n\n\n2.4.5 Random Forest\nThe following code was used in Research Question 4 Random Forest analysis.\n\n# ---------------------------\n# Random Forest analysis\n# ---------------------------\ntrain_rf &lt;- data.frame(waist = y_train, x_train_sc)\ntest_rf  &lt;- data.frame(waist = y_test,  x_test_sc)\n\nfold_id_rf &lt;- fold_id\ncv_rf &lt;- data.frame(fold = 1:K, rmse = NA, mae = NA, rsq = NA)\n\nfor (k in 1:K) {\n  tr_idx &lt;- which(fold_id_rf != k)\n  va_idx &lt;- which(fold_id_rf == k)\n  \n  set.seed(25 + k)\n  rf_k &lt;- randomForest(\n    waist ~ .,\n    data = train_rf[tr_idx, ],\n    ntree = 1000,\n    importance = TRUE\n  )\n  \n  pred &lt;- predict(rf_k, newdata = train_rf[va_idx, ])\n  \n  cv_rf$rmse[k] &lt;- rmse(train_rf$waist[va_idx], pred)\n  cv_rf$mae[k]  &lt;- mae(train_rf$waist[va_idx], pred)\n  cv_rf$rsq[k]  &lt;- rsq(train_rf$waist[va_idx], pred)\n}\n\nkable(\n  data.frame(\n    rmse_mean = mean(cv_rf$rmse), rmse_sd = sd(cv_rf$rmse),\n    mae_mean  = mean(cv_rf$mae),  mae_sd  = sd(cv_rf$mae),\n    rsq_mean  = mean(cv_rf$rsq),  rsq_sd  = sd(cv_rf$rsq)\n  ),\n  caption = \"RQ4 - Random Forest CV mean +/- sd\",\n  align = \"c\"\n)\n\n\nRQ4 - Random Forest CV mean +/- sd\n\n\nrmse_mean\nrmse_sd\nmae_mean\nmae_sd\nrsq_mean\nrsq_sd\n\n\n\n\n16.08423\n0.3754279\n12.61172\n0.2918258\n0.1250506\n0.0291024\n\n\n\n\nrf_final &lt;- randomForest(\n  waist ~ .,\n  data = train_rf,\n  ntree = 1000,\n  importance = TRUE\n)\n\nrf_pred &lt;- predict(rf_final, newdata = test_rf)\n\nkable(\n  data.frame(\n    rmse = rmse(y_test, rf_pred),\n    mae  = mae(y_test, rf_pred),\n    rsq  = rsq(y_test, rf_pred)\n    ),\n  caption = \"RQ4 - Test metrics (Random Forest)\",\n  align = \"c\"\n)\n\n\nRQ4 - Test metrics (Random Forest)\n\n\nrmse\nmae\nrsq\n\n\n\n\n15.57457\n12.24861\n0.0998489\n\n\n\n\n(rf_imp_mat &lt;- importance(rf_final))\n\n                   %IncMSE IncNodePurity\nage              45.730610    162524.312\nsexFemale        37.794572     26812.319\nrace2            11.390678      8161.818\nrace3            17.196276     16477.883\nrace4            19.286635     17368.652\nrace6            90.998273     53460.400\nrace7            -8.708870      8351.698\neducation2        3.215864     10019.030\neducation3       12.701908     13516.310\neducation4       18.631880     14504.783\neducation5       22.430658     14127.158\nindfmpir         23.300287    148046.060\nmvpa_equiv_minwk 24.948552    120836.645\n\nrf_imp_df &lt;- as.data.frame(rf_imp_mat) %&gt;%\n  mutate(feature = rownames(rf_imp_mat)) %&gt;%\n  mutate(importance = if (\"`%IncMSE`\" %in% names(.)) `\"%IncMSE\"` else IncNodePurity) %&gt;%\n  select(feature, importance) %&gt;%\n  arrange(desc(importance)) %&gt;%\n  mutate(method = \"RF_importance\")\n\nkable(head(rf_imp_df, 20),\n      caption = \"RQ4 - Top 20 RF important features\",\n      align = \"c\")\n\n\nRQ4 - Top 20 RF important features\n\n\n\nfeature\nimportance\nmethod\n\n\n\n\nage\nage\n162524.312\nRF_importance\n\n\nindfmpir\nindfmpir\n148046.060\nRF_importance\n\n\nmvpa_equiv_minwk\nmvpa_equiv_minwk\n120836.645\nRF_importance\n\n\nrace6\nrace6\n53460.400\nRF_importance\n\n\nsexFemale\nsexFemale\n26812.319\nRF_importance\n\n\nrace4\nrace4\n17368.652\nRF_importance\n\n\nrace3\nrace3\n16477.883\nRF_importance\n\n\neducation4\neducation4\n14504.783\nRF_importance\n\n\neducation5\neducation5\n14127.158\nRF_importance\n\n\neducation3\neducation3\n13516.310\nRF_importance\n\n\neducation2\neducation2\n10019.030\nRF_importance\n\n\nrace7\nrace7\n8351.698\nRF_importance\n\n\nrace2\nrace2\n8161.818\nRF_importance\n\n\n\n\n# Base plot\n#varImpPlot(rf_final, n.var = 20, main = \"RF Variable Importance (Top 20)\")\n\n\n\n2.4.6 Comparison\nThe following code was used in Research Question 4 comparison of top predictors across methods.\n\n# ---------------------------\n# Consistency summary: compare top predictors across methods\n# ---------------------------\ntop_n &lt;- 15\n\nrank_table &lt;- bind_rows(\n  mlr_imp %&gt;% slice_head(n = top_n),\n  ridge_imp %&gt;% slice_head(n = top_n),\n  lasso_imp %&gt;% slice_head(n = top_n),\n  rf_imp_df %&gt;% slice_head(n = top_n)\n) %&gt;%\n  group_by(method) %&gt;%\n  mutate(rank = row_number()) %&gt;%\n  ungroup()\n\ncat(\"\\nRQ4 - Top predictors by method (top 15 each):\\n\")\n\n\nRQ4 - Top predictors by method (top 15 each):\n\nprint(rank_table)\n\n# A tibble: 52 × 4\n   feature                    importance method            rank\n   &lt;chr&gt;                           &lt;dbl&gt; &lt;chr&gt;            &lt;int&gt;\n 1 x_train_scrace6                 4.39  MLR_abs_std_coef     1\n 2 x_train_scsexFemale             2.14  MLR_abs_std_coef     2\n 3 x_train_scage                   2.13  MLR_abs_std_coef     3\n 4 x_train_sceducation4            1.71  MLR_abs_std_coef     4\n 5 x_train_scrace2                 1.20  MLR_abs_std_coef     5\n 6 x_train_sceducation3            1.05  MLR_abs_std_coef     6\n 7 x_train_scmvpa_equiv_minwk      0.943 MLR_abs_std_coef     7\n 8 x_train_scrace4                 0.871 MLR_abs_std_coef     8\n 9 x_train_scrace3                 0.425 MLR_abs_std_coef     9\n10 x_train_scrace7                 0.302 MLR_abs_std_coef    10\n# ℹ 42 more rows\n\nconsensus &lt;- rank_table %&gt;%\n  group_by(feature) %&gt;%\n  summarize(\n    methods_present = paste(sort(unique(method)), collapse = \" | \"),\n    n_methods = n_distinct(method),\n    best_rank = min(rank),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(desc(n_methods), best_rank)\n\ncat(\"\\nRQ4 - Cross-method consensus (features appearing in multiple \n    top-15 lists):\\n\")\n\n\nRQ4 - Cross-method consensus (features appearing in multiple \n    top-15 lists):\n\nprint(consensus)\n\n# A tibble: 26 × 4\n   feature          methods_present                          n_methods best_rank\n   &lt;chr&gt;            &lt;chr&gt;                                        &lt;int&gt;     &lt;int&gt;\n 1 age              Lasso_abs_coef | RF_importance | Ridge_…         3         1\n 2 race6            Lasso_abs_coef | RF_importance | Ridge_…         3         1\n 3 indfmpir         Lasso_abs_coef | RF_importance | Ridge_…         3         2\n 4 sexFemale        Lasso_abs_coef | RF_importance | Ridge_…         3         2\n 5 mvpa_equiv_minwk Lasso_abs_coef | RF_importance | Ridge_…         3         3\n 6 education4       Lasso_abs_coef | RF_importance | Ridge_…         3         4\n 7 race2            Lasso_abs_coef | RF_importance | Ridge_…         3         5\n 8 race4            Lasso_abs_coef | RF_importance | Ridge_…         3         6\n 9 education3       Lasso_abs_coef | RF_importance | Ridge_…         3         7\n10 race3            Lasso_abs_coef | RF_importance | Ridge_…         3         7\n# ℹ 16 more rows\n\nimp_all &lt;- bind_rows(\n  mlr_imp %&gt;% mutate(method = \"MLR\"),\n  ridge_imp %&gt;% mutate(method = \"Ridge\"),\n  lasso_imp %&gt;% mutate(method = \"Lasso\"),\n  rf_imp_df %&gt;% mutate(method = \"Random Forest\")\n) %&gt;%\n  group_by(method) %&gt;%\n  slice_max(order_by = importance, n = top_n, with_ties = FALSE) %&gt;%\n  ungroup()\n\nggplot(imp_all, aes(x = reorder(feature, importance), y = importance)) +\n  geom_col() +\n  coord_flip() +\n  facet_wrap(~ method, scales = \"free\") +\n  labs(\n    title = \"Top Predictors of Waist Circumference by Modeling Technique\",\n    x = NULL,\n    y = \"Importance (method-specific scale)\"\n  )\n\n\n\n\n\n\n\nfocus_feats &lt;- consensus %&gt;%\n  filter(n_methods &gt;= 2) %&gt;%\n  arrange(desc(n_methods), best_rank) %&gt;%\n  slice_head(n = 25) %&gt;%\n  pull(feature)\n\nrank_heat &lt;- rank_table %&gt;%\n  filter(feature %in% focus_feats) %&gt;%\n  select(method, feature, rank)\n\nggplot(rank_heat, aes(x = method, y = feature, fill = rank)) +\n  geom_tile(color = \"white\") +\n  scale_y_discrete(limits = rev) +\n  labs(\n    title = \"Cross-Model Consistency: Predictor Rank (Lower = More Important)\",\n    x = NULL, y = NULL, fill = \"Rank\"\n  )\n\n\n\n\n\n\n\ncv_perf &lt;- bind_rows(\n  cv_mlr %&gt;% mutate(method = \"MLR\"),\n  cv_rf  %&gt;% mutate(method = \"Random Forest\")\n)\n\nggplot(cv_perf, aes(x = method, y = rmse)) +\n  geom_boxplot() +\n  labs(title = \"Cross-Validation RMSE by Method\", x = NULL, y = \"RMSE (cm)\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "This website was created using Quarto https://quarto.org and Rstudio for my STAT515 class at GMU. Check back often for updates!"
  },
  {
    "objectID": "index.html#welcome-to-my-page",
    "href": "index.html#welcome-to-my-page",
    "title": "Welcome",
    "section": "",
    "text": "This website was created using Quarto https://quarto.org and Rstudio for my STAT515 class at GMU. Check back often for updates!"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "LinkedIn - https://www.linkedin.com/in/blake-lafever/\n\n\nE-mail - blafever@gmu.edu"
  },
  {
    "objectID": "final_proj.html",
    "href": "final_proj.html",
    "title": "A Multimodel Analysis of Income, Physical Activity, and Body Composition in U.S. Adults",
    "section": "",
    "text": "Obesity and central adiposity are persistent public health concerns in the United States and are strongly associated with elevated risk of cardiovascular disease, type 2 diabetes, and other chronic conditions. Body mass index (BMI) is widely used as a population-level indicator of obesity; however, waist circumference provides complementary information by capturing abdominal fat distribution, which is more directly linked to cardiovascular risk. As a result, both measures are commonly employed in epidemiological and clinical research to assess obesity-related health outcomes.\nThe availability of large, nationally representative datasets enables more comprehensive investigation of these relationships. The National Health and Nutrition Examination Survey (NHANES) integrates detailed demographic information, socioeconomic indicators, self-reported physical activity measures, and objectively collected body measurements [1-3]. This structure supports joint evaluation of multiple predictors while controlling for potential confounding factors and allows for comparison of results across different measured outcomes.\n\n\nThe objective of this study is to assess how demographic, socioeconomic, and physical activity variables are associated with obesity-related outcomes using NHANES 2017–2018 data [4-6]. Four research questions guide the analysis. First, the study evaluates the extent to which daily sitting time and age predict BMI and whether including sex and/or interaction effects improves model performance. Second, it examines the association between household income and both BMI as a continuous variable and the probability of being classified as obese, while controlling for key demographic characteristics. Third, the study investigates which demographic variables best explain variation in total leisure-time physical activity using alternative modeling techniques. Finally, it evaluates which demographic and physical activity factors are most strongly associated with waist circumference and assesses the consistency of these associations across modeling approaches.\n\n\n\nThis study emphasizes methodological robustness by applying multiple regression-based and machine learning models and evaluating performance using cross-validation. By comparing results across continuous and categorical outcomes and across BMI and waist circumference, the analysis provides insight into whether key predictors operate consistently across different measures of adiposity. The findings contribute to a clearer understanding of obesity-related disparities and highlight the importance of using complementary outcomes and modeling techniques when analyzing complex population health data."
  },
  {
    "objectID": "final_proj.html#introduction",
    "href": "final_proj.html#introduction",
    "title": "A Multimodel Analysis of Income, Physical Activity, and Body Composition in U.S. Adults",
    "section": "",
    "text": "Obesity and central adiposity are persistent public health concerns in the United States and are strongly associated with elevated risk of cardiovascular disease, type 2 diabetes, and other chronic conditions. Body mass index (BMI) is widely used as a population-level indicator of obesity; however, waist circumference provides complementary information by capturing abdominal fat distribution, which is more directly linked to cardiovascular risk. As a result, both measures are commonly employed in epidemiological and clinical research to assess obesity-related health outcomes.\nThe availability of large, nationally representative datasets enables more comprehensive investigation of these relationships. The National Health and Nutrition Examination Survey (NHANES) integrates detailed demographic information, socioeconomic indicators, self-reported physical activity measures, and objectively collected body measurements [1-3]. This structure supports joint evaluation of multiple predictors while controlling for potential confounding factors and allows for comparison of results across different measured outcomes.\n\n\nThe objective of this study is to assess how demographic, socioeconomic, and physical activity variables are associated with obesity-related outcomes using NHANES 2017–2018 data [4-6]. Four research questions guide the analysis. First, the study evaluates the extent to which daily sitting time and age predict BMI and whether including sex and/or interaction effects improves model performance. Second, it examines the association between household income and both BMI as a continuous variable and the probability of being classified as obese, while controlling for key demographic characteristics. Third, the study investigates which demographic variables best explain variation in total leisure-time physical activity using alternative modeling techniques. Finally, it evaluates which demographic and physical activity factors are most strongly associated with waist circumference and assesses the consistency of these associations across modeling approaches.\n\n\n\nThis study emphasizes methodological robustness by applying multiple regression-based and machine learning models and evaluating performance using cross-validation. By comparing results across continuous and categorical outcomes and across BMI and waist circumference, the analysis provides insight into whether key predictors operate consistently across different measures of adiposity. The findings contribute to a clearer understanding of obesity-related disparities and highlight the importance of using complementary outcomes and modeling techniques when analyzing complex population health data."
  },
  {
    "objectID": "final_proj.html#data-description",
    "href": "final_proj.html#data-description",
    "title": "A Multimodel Analysis of Income, Physical Activity, and Body Composition in U.S. Adults",
    "section": "2 Data Description",
    "text": "2 Data Description\n\n2.1 Data Source\nChatGPT [7] was used to assist with narrowing down a dataset to use for this project, in addition to, report structuring and explanation; all analysis and interpretation were conducted by the author. The data used in this study were obtained from the National Health and Nutrition Examination Survey (NHANES) 2017–2018 cycle [1-3]. NHANES is a continuous, cross-sectional survey conducted by the National Center for Health Statistics designed to assess the health and nutritional status of the civilian, non-institutionalized U.S. population. The survey employs a complex, multistage probability sampling design and combines in-home interviews with standardized physical examinations conducted in mobile examination centers.\nThree publicly available NHANES datasets were used in this analysis: the Demographics file (DEMO_J) [4], the Body Measures file (BMX_J) [5], and the Physical Activity Questionnaire file (PAQ_J) [6]. These datasets were merged using the unique participant identifier (SEQN), resulting in a unified analytical dataset containing demographic, socioeconomic, and physical activity variables.\n\n\n2.2 Study Population\nThe analytical sample consists of adult participants aged 18 years and older who had non-missing data for the primary outcome variables and key predictors of interest. Participants with incomplete or implausible values for body measurements or physical activity variables were excluded from relevant analyses. Because NHANES data are cross-sectional, all observations represent a single point in time for each participant.\n\n\n2.3 Key Variables\nMeasurement outcomes were obtained from the Body Measures dataset. Body mass index (BMI) was calculated by NHANES using measured height and weight and treated as a continuous outcome in regression analyses. Obesity status was defined using standard clinical thresholds (BMI ≥ 30 kg/m²) and modeled as a binary outcome where applicable. Waist circumference, measured in centimeters, was used as an additional continuous outcome representing central adiposity.\nSocioeconomic status was assessed using household income variables from the Demographics dataset. Poverty-Income Ratio was utilized as a reference for the income variable. Demographic variables included age, sex, and race/ethnicity, all of which were self-reported. Physical activity variables were derived from the Physical Activity Questionnaire dataset. Measures included self-reported leisure-time physical activity and daily sedentary behavior, such as time spent sitting. These variables were used both as primary predictors and as control variables depending on the specific research question being evaluated.\n\n\n2.4 Data Preparation\nPrior to analysis, datasets were cleaned and merged using SEQN. Variables were recoded as necessary to ensure consistent units and meaningful reference categories. Continuous variables were examined for outliers and implausible values, and categorical variables were converted to factor representations where appropriate. Missing data were handled using complete-case analysis within each modeling task, such that only participants with available data for the variables included in a given model were retained.\nSurvey weights were not applied in this analysis, as the primary objective was to evaluate associations and comparative model performance rather than to produce nationally representative prevalence estimates. All analyses were conducted using the cleaned and merged NHANES 2017–2018 dataset."
  },
  {
    "objectID": "final_proj.html#research-questions",
    "href": "final_proj.html#research-questions",
    "title": "A Multimodel Analysis of Income, Physical Activity, and Body Composition in U.S. Adults",
    "section": "3 Research Questions",
    "text": "3 Research Questions\n\n3.1 Question 1\nHow well do daily sitting time and age predict BMI, and does adding sex (and potentially an interaction term i.e. sitting time × sex) meaningfully improve the model’s predictive performance?\n\n\n3.2 Question 2\nHow strongly is household income associated with (a) BMI as a continuous outcome and (b) the probability of being classified as obese, after controlling for key demographic variables?\n\n\n3.3 Question 3\nWhich demographic variables best explain variation in total leisure-time physical activity, and how do predictor rankings differ when using alternative modeling techniques?\n\n\n3.4 Question 4\nWhich demographic and physical-activity factors are most strongly associated with waist circumference, and how consistent are these associations when evaluated using different modeling techniques?"
  },
  {
    "objectID": "final_proj.html#methodology",
    "href": "final_proj.html#methodology",
    "title": "A Multimodel Analysis of Income, Physical Activity, and Body Composition in U.S. Adults",
    "section": "4 Methodology",
    "text": "4 Methodology\n\n4.1 RQ1\nOur first research question was answered by performing multiple linear regression using the Body Mass Index (BMXBMI) as the outcome and sedentary activity minutes (PAD680) and age (RIDAGEYR) as the predictors.\nThe initial baseline model only included PAD680 and RIDAGEYR. The second model incorporates gender (RIAGENDR). To further explore the data, we created a third MLR model also including an interaction term between RIAGENDR and PAD680.\nThe model’s performance was assessed using k-fold cross validation.\n\n\n4.2 RQ2\nOur second research question was answered by using a few different models.\nBody Mass Index (BMXBMI) was used both as a continuous outcome and a binary obesity indicator defined as BMXBMI ≥ 30. Family income-to-poverty ratio (INDFMPIR) served as the primary predictor while age (RIDAGEYR), sex (RIAGENDR), race/ethnicity (RIDRETH3), and education level (DMDEDUC2) acted as the covariates. Both unadjusted models (income only) and adjusted models (including demographic covariates) were estimated. Model performance was assessed using k-fold cross-validation.\nLogistic regression was used to model the probability of obesity. Odds ratios and 95% confidence intervals were reported to quantify the association between income and obesity risk.\nRidge and Lasso regression were additionally applied to the BMI outcome to evaluate model stability, address multicollinearity, and assess relative predictor importance. Regularization parameters were selected using cross-validation.\n\n\n4.3 RQ3\nThe third research question focuses on demographic indicators and how they might affect physical activity leisure time. To answer this question, a few data cleaning steps had to be completed.\nWe created a subset of the dataset to help address multicollinearity and created a calculated column to standardize physical activity leisure time into minutes per week (PAD660 * PAQ655) + (PAD675 * PAQ670). The variables used are vigorous recreational activity minutes (PAD660), days of vigorous recreational activities (PAQ655), moderate recreational activity minutes (PAQ655), and days of moderate recreational activities (PAQ670).\nAlthough we created a subset of the data, there were still several variables that could potentially cause multicollinearity issues. We used the LASSO model for further variable selection and the Ridge model to help stabilize coefficient estimates with correlated predictors. To also account for potential nonlinear relationships and variable importance rankings, we used a Random Forest model. Model performance was compared across all approaches and cross-validated.\n\n\n4.4 RQ4\nMultiple Linear Regression models were fit to examine linear associations between waist circumference and demographic and physical-activity predictors. A demographic-only baseline model was compared with a full model including physical-activity variables.\nThe variables used for these models were waist circumference (BMXWAIST) serving as the outcome variable and age (RIDAGEYR), sex (RIAGENDR), race/ethnicity (RIDRETH3), education (DMDEDUC2), and family income-to-poverty ratio (INDFMPIR) as the demographic predictors.\nRidge and Lasso regression were used to evaluate consistency and importance of predictors under regularization. Random forest was used to capture potential nonlinear relationships and interactions and to generate variable importance rankings.\nAll models were evaluated using k-fold cross-validation with root mean squared error as the primary metric."
  },
  {
    "objectID": "final_proj.html#resultsanalysis",
    "href": "final_proj.html#resultsanalysis",
    "title": "A Multimodel Analysis of Income, Physical Activity, and Body Composition in U.S. Adults",
    "section": "5 Results/Analysis",
    "text": "5 Results/Analysis\n\n5.1 RQ1\n\n\nTable 1. Basic Model Summary Data\n\n\n\nCall:\nlm(formula = BMXBMI ~ PAD680 + RIDAGEYR, data = health)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-17.896  -5.110  -1.232   3.761  56.783 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 2.900e+01  2.917e-01  99.399   &lt;2e-16 ***\nPAD680      3.291e-04  1.358e-04   2.423   0.0154 *  \nRIDAGEYR    1.144e-02  5.443e-03   2.102   0.0356 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.436 on 5422 degrees of freedom\n  (3829 observations deleted due to missingness)\nMultiple R-squared:  0.001962,  Adjusted R-squared:  0.001594 \nF-statistic: 5.329 on 2 and 5422 DF,  p-value: 0.004872\n\n\nWe performed multiple linear regression using BMI (BMXBMI), number of minutes of sedentary activity (PAD680), and age (RIDAGEYR). PAD680 had a positive coefficient in relation to BMXBMI, indicating that more time being sedentary was associated with slightly higher BMI. It’s p-value is also 0.0154, sitting below the 0.05 marker, deeming it to be statistically significant.\nRIDAGEYR also had similar results. It had a positive coefficient, indicating that older age is associated with slightly higher BMI. The p-value is also small enough to say that it is statistically significant at 0.0356. Our adjusted R-squared is low (0.001594), indicating that this model is not a good fit for predicting BMI.\n\n\nTable 2. Basic Model + Gender Summary Data\n\n\n\nCall:\nlm(formula = BMXBMI ~ PAD680 + RIDAGEYR + RIAGENDR, data = health)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-17.329  -5.089  -1.261   3.811  57.252 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 2.851e+01  3.110e-01  91.689  &lt; 2e-16 ***\nPAD680      3.178e-04  1.356e-04   2.344   0.0191 *  \nRIDAGEYR    1.194e-02  5.435e-03   2.197   0.0281 *  \nRIAGENDR2   8.982e-01  2.018e-01   4.451  8.7e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.424 on 5421 degrees of freedom\n  (3829 observations deleted due to missingness)\nMultiple R-squared:  0.005597,  Adjusted R-squared:  0.005047 \nF-statistic: 10.17 on 3 and 5421 DF,  p-value: 1.121e-06\n\n\nIn this model, we are adding gender (RIAGENDR) into the multiple linear regression model. RIAGENDR had a much larger coefficient with BMXBMI, indicating that females have 0.9 higher BMI than males when including age and sedentary time into the model. It’s p-value is much smaller than PAD680 or RIDAGEYR, definitely sitting below the 0.05 threshold, meaning it is statistically significant. The adjusted R-squared is slightly better at 0.005047, but still very low, indicating that this is another poor fit for predicting BMI.\n\n\nTable 3. Basic Model + Gender + Interaction Summary Data\n\n\n\nCall:\nlm(formula = BMXBMI ~ RIDAGEYR + RIAGENDR + PAD680 + RIAGENDR:PAD680, \n    data = health)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.170  -5.097  -1.254   3.800  57.180 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      2.860e+01  3.181e-01  89.900  &lt; 2e-16 ***\nRIDAGEYR         1.199e-02  5.434e-03   2.207 0.027375 *  \nRIAGENDR2        7.626e-01  2.289e-01   3.331 0.000872 ***\nPAD680           8.370e-05  2.308e-04   0.363 0.716853    \nRIAGENDR2:PAD680 3.573e-04  2.850e-04   1.253 0.210082    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.423 on 5420 degrees of freedom\n  (3829 observations deleted due to missingness)\nMultiple R-squared:  0.005885,  Adjusted R-squared:  0.005151 \nF-statistic: 8.021 on 4 and 5420 DF,  p-value: 1.916e-06\n\n\nIn this model, we are adding an interaction term with RIAGENDR and PAD680, along with their main effects. The goal of adding this interaction term was to test PAD680’s relationship with BMI and if it differed by sex.\nThe RIAGENDR variable stood out with its 0.000872 p-value, indicating a difference in BMI between males and females.\nAfter adding the interaction term, many of the coefficients changed; RIAGENDR’s coefficient shrank to 0.7625913 and PAD680’s shrank an order of magnitude. The adjusted R-squared slightly increased to 0.005151; this model explains only a small amount of the variation in BMI, making it a poor predictor for BMI.\n\n\nTable 4. Cross-validation Summary Data\n\n\n\nCall:\nlm(formula = .outcome ~ ., data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.170  -5.097  -1.254   3.800  57.180 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        2.860e+01  3.181e-01  89.900  &lt; 2e-16 ***\nPAD680             8.370e-05  2.308e-04   0.363 0.716853    \nRIDAGEYR           1.199e-02  5.434e-03   2.207 0.027375 *  \nRIAGENDR2          7.626e-01  2.289e-01   3.331 0.000872 ***\n`PAD680:RIAGENDR2` 3.573e-04  2.850e-04   1.253 0.210082    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.423 on 5420 degrees of freedom\nMultiple R-squared:  0.005885,  Adjusted R-squared:  0.005151 \nF-statistic: 8.021 on 4 and 5420 DF,  p-value: 1.916e-06\n\n\n\n\n5.2 RQ2\n\n\n\nTable 5. Summary Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\nbmi\nage\nsex\nrace\nindfmpir\neducation\nmvpa_equiv_minwk\n\n\n\n\n\nMin. :14.20\nMin. :20.00\nMale :2162\n1: 562\nMin. :0.000\n1: 332\nMin. : 0.0\n\n\n\n1st Qu.:24.70\n1st Qu.:36.00\nFemale:2318\n2: 389\n1st Qu.:1.200\n2: 494\n1st Qu.: 0.0\n\n\n\nMedian :28.60\nMedian :52.00\nNA\n3:1635\nMedian :2.150\n3:1081\nMedian : 240.0\n\n\n\nMean :29.83\nMean :51.15\nNA\n4:1017\nMean :2.557\n4:1476\nMean : 985.8\n\n\n\n3rd Qu.:33.60\n3rd Qu.:65.00\nNA\n6: 645\n3rd Qu.:4.080\n5:1097\n3rd Qu.: 1080.0\n\n\n\nMax. :86.20\nMax. :80.00\nNA\n7: 232\nMax. :5.000\nNA\nMax. :14580.0\n\n\n\n\n\nTable 5 shows summary statistics for the given predictors that will be used in subsequent analysis.\n\n\n\n\n\nFigure 1. Pair-wise comparison of selected predictors\n\n\n\n\n\n\n\n\nFigure 2. Distribution of BMI\n\n\n\n\n\n\n\nFigure 3. Scatterplot of Poverty-Income Ration vs BMI\n\n\n\n\nFigure 1 shows the pair-wise comparison between each of the selected predictors and relative magnitude of correlation. Figure 2 show the distribution of BMI across Poverty-Income ratios. Figure 3 shows the scatterplot distribution of Figure 2.\nRQ2 was separated into two parts, for ease of analysis and interpretation. The first will use MLR, Ridge Regression, and Lasso Regression for analysis and comparison. The second will use Logistic Regression for analysis.\n\n5.2.1 Part (a)\nUsing MLR and cross-validation, we receive the results shown in Tables 6 and 7.\n\n\n\nTable 6. MLR CV metrics\n\n\nrmse_mean\nrmse_sd\nmae_mean\nmae_sd\nrsq_mean\nrsq_sd\n\n\n\n\n7.234845\n0.2995047\n5.394304\n0.2391558\n0.055296\n0.0110509\n\n\n\n\n\n\n\n\nTable 7. Results when applying MLR model to Test dataset\n\n\nrmse\nmae\nrsq\n\n\n\n\n7.085617\n5.449196\n0.045798\n\n\n\n\n\nThe RMSE and MAE values when using the test data are in line with the mean values from CV. RSQ is slightly different between the two indicating that the model performed worse on the test data than on the CV.\nLooking at the summary for the model in Table 8, we note that the two most significant predictors here are race category 6 and moderate-vigorous physical activity. However, we find that the model only fits roughly 6% of the variability of the data, making it a poor fit. Other combinations of predictors did not provide any improvement to the fit of the model to data.\n\n\nTable 8. MLR Model Summary Data\n\n\n\nCall:\nlm(formula = bmi ~ indfmpir + age + sex + race + education + \n    mvpa_equiv_minwk, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.376  -4.776  -0.864   3.599  54.275 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       3.170e+01  6.733e-01  47.074  &lt; 2e-16 ***\nindfmpir         -1.922e-04  8.740e-02  -0.002   0.9982    \nage              -1.409e-02  7.311e-03  -1.927   0.0541 .  \nsexFemale         5.451e-01  2.475e-01   2.202   0.0277 *  \nrace2            -1.281e+00  5.358e-01  -2.392   0.0168 *  \nrace3            -1.036e+00  4.173e-01  -2.482   0.0131 *  \nrace4             2.908e-01  4.433e-01   0.656   0.5119    \nrace6            -4.830e+00  4.969e-01  -9.720  &lt; 2e-16 ***\nrace7            -6.333e-01  6.423e-01  -0.986   0.3243    \neducation2       -7.049e-01  5.868e-01  -1.201   0.2297    \neducation3        2.945e-01  5.341e-01   0.552   0.5813    \neducation4        6.460e-01  5.321e-01   1.214   0.2248    \neducation5       -6.967e-01  5.769e-01  -1.208   0.2273    \nmvpa_equiv_minwk -3.255e-04  7.526e-05  -4.325 1.56e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.229 on 3570 degrees of freedom\nMultiple R-squared:  0.06296,   Adjusted R-squared:  0.05955 \nF-statistic: 18.45 on 13 and 3570 DF,  p-value: &lt; 2.2e-16\n\n\nSimilarly, as shown in Figure 4, the change in BMI across Income levels is negligible, indicating no correlation between the two after correcting for other demographic factors. (Note the y-axis scale)\n\n\n\n\n\nFigure 4. Plot of predicted BMI based on Poverty-Income Ratio\n\n\n\n\nPerforming Ridge and Lasso regressions, we obtain the following results in Tables 9 and 10.\n\n\n\nTable 9. Ridge performance metrics against test dataset\n\n\nrmse\nmae\nrsq\n\n\n\n\n7.082975\n5.448042\n0.0465095\n\n\n\n\n\n\n\n\nTable 10. Lasso performance metrics against test dataset\n\n\nrmse\nmae\nrsq\n\n\n\n\n7.076227\n5.444314\n0.0483253\n\n\n\n\n\nHere we see similar values to those shown in the MLR analysis. Again, RSQ is low indicating these models are poor fits to describe the dataset.\nNext, in Figure 5, we perform a comparison between each model and show the relative magnitude of each income variable as it relates to BMI. Each has a relative small (in the Lasso case zero) effect on the change to BMI.\n\n\n\n\n\nFigure 5. Income Coefficient Across Modeling Techniques\n\n\n\n\nOverall, the results presented for part (a) do not indicate that household income is strongly correlated with BMI.\n\n\n5.2.2 Part (b)\n\n\n\n\n\nFigure 6. Obesity Prevalence by Household Income Group\n\n\n\n\nFigure 6 displays the results after breaking BMI numbers into two groups, Obese or Not Obese.\nFor this part of the question, we’ll be using a Logistic Regression with cross-validation. Table 11 outlines the results from the CV results as well as the model with test data.\n\n\nTable 11. Logistic Regression with Cross Validation\n\n\n   acc_mean     acc_sd sens_mean    sens_sd spec_mean    spec_sd prec_mean\n1 0.5853903 0.02385022 0.3027356 0.03990417 0.7901275 0.03446358 0.5104574\n     prec_sd   f1_mean      f1_sd balacc_mean  balacc_sd\n1 0.04315099 0.3785182 0.03668177   0.5464316 0.02183762\n\n\nOf these metrics, the most signficant is the balanced accuracy (sensitivity + specificity). Here, we see 54.6% balanced accuracy rate, which is only slightly above a 50/50 chance. This means that the model provides only slightly better odds than random categorization. Additionally, the low f1 score confirms weak detection of obesity cases.\nFigure 7 provides the predicted probability of being obese based on a specified Poverty-Income ratio score. Interestingly, this model shows that as income goes up (higher ratio number), predicted probability of obesity also goes up. The magnitude of the change isn’t large, however, it does trend in a positive direction.\n\n\n\n\n\nFigure 7. Probability of Obesity by Household Income\n\n\n\n\nFigure 8 provides results for OR and CIs for the Logistic Regression on a log-odds scale. Here, our income odds being slightly larger than 1 indicate that income has a statistically detectable association with obesity risk, but its effect size is insufficient for accurate individual classification.\n\n\n\n\n\nFigure 8. Odds Ratios for Obesity (Log Reg)\n\n\n\n\n\n\n\n5.3 RQ3\n\n\n\n\n\nFigure 9. Pair-wise Plot of Selected Predictors\n\n\n\n\n\n\n\n\n\nFigure 10. Lasso Plot\n\n\n\n\n\n\nTable 12. Lasso Model Metrics\n\n\n[1] 425.9082\n\n\n (Intercept)    RIAGENDR2     RIDAGEYR     RIDRETH3     DMQMILIZ     DMDCITZN \n490.39291448 -22.06657262  -0.70797843   0.06584831   0.00000000   0.00000000 \n   DMDEDUC22    DMDEDUC23    DMDEDUC24    DMDEDUC25    DMDEDUC27    DMDEDUC29 \n105.17706751   0.00000000   0.00000000 -38.72220774   0.00000000   0.00000000 \n\n\nIn the LASSO model, we use the RMSE to obtain the typical prediction error in minutes, between the observed and predicted outcomes; the model was off by approximately 426 minutes.\nThrough the coefficient analysis, we can see some of the variable coefficients LASSO has zeroed out, such as military participation (DMQMILIZ), some of the education groups (DMDEDUC24, DMDEDUC27, and DMDEDUC29). Through the same model, DMDEDUC22 has a coefficient score of 105.177 and sex (RIAGENDR) has a coefficient score of -22.067. From this model, we can interpret that adults that have an education level between 9th and 12th grade (no diploma), are predicted to have 105 more physical activity minutes. We can also interpret that women are predicted to have 22 minutes less than males based on the coefficient analysis.\n\n\n\n\n\nFigure 11. Ridge Regression Plot\n\n\n\n\n\n\nTable 13. Ridge Model Metrics\n\n\n[1] 417.0286\n\n\nIn the Ridge Regression model, we use the RSME to obtain the typical prediction error in minutes, between the observed and predicted outcomes; the model was off by approximately 417 minutes, the most accurate model so far.\nIn the coefficient analysis, we can see that DMDEDUC22 and RIAGENDR are still regarded as the most impactful predictors. The only coefficients that Ridge zeroed out were DMDEDUC27 and DMDEDUC29, the refused and don’t know groups, respectively. For adults that had an education level between 9th and 12th grade (no diploma), they were predicted to have 63 more minutes of physical activity during their leisure time. The RIAGENDR coefficient shrank to -20; from this, we can interpret that women are predicted to have 20 less minutes of physical activity leisure time per week compared to men.\n\n\nTable 14. Ridge Coefficients\n\n\n(Intercept)   RIAGENDR2    RIDAGEYR    RIDRETH3    DMQMILIZ    DMDCITZN \n500.2858184 -20.0262810  -0.6231719   3.0257839  -1.2562350  -3.3760809 \n  DMDEDUC22   DMDEDUC23   DMDEDUC24   DMDEDUC25   DMDEDUC27   DMDEDUC29 \n 62.8706004  16.6736615   4.3829060 -22.8309922   0.0000000   0.0000000 \n\n\n\n\nTable 15. Random Forest Metrics\n\n\n[1] 183267.7\n\n\n[1] 428.0978\n\n\n\n\nTable 16. Importance Metrics\n\n\n             %IncMSE IncNodePurity\nRIAGENDR -0.68545060     1227172.3\nRIDAGEYR  0.04963857     7450183.5\nRIDRETH3  1.78743634     3834674.0\nDMQMILIZ  0.85243169      258252.1\nDMDCITZN  2.17401275      941210.1\nDMDEDUC2  5.63626197     5426340.5\nDMDMARTL  0.55017530     3569002.6\nDMDBORN4  3.35225882     1115497.6\nINDHHIN2 -0.17057506     5776478.1\nINDFMIN2 -2.16243290     5493280.9\nINDFMPIR  1.81024071     7027641.2\n\n\n\n\n\n\n\nFigure 12. Random Forest Importance Plot\n\n\n\n\nIn this model, we are using the Random Forest model, importance() function, and the plot to see which demographic variables are the most impactful when predicting physical activity leisure time minutes. Using the RMSE, the model was off by approximately 428 minutes.\nBased off of the plot, we can see that DMDEDUC2, INDFMPIR, and RIDRETH3 were the 3 most impactful variables to the model. The right plot shows which variables were used the most to split; the top 3 being RIDAGEYR, INDFMPIR,and INDHHIN2. This model performed the worst, and also placed RIAGENDR closer to the middle of the importance spectrum.\n\n\n5.4 RQ4\n\n\n\n\n\nFigure 13. Histogram w/ Density of Waist Circumference\n\n\n\n\n\n\n\n\n\nFigure 14. Waist Circumference vs MVPA\n\n\n\n\n\n\n\n\n\nFigure 15. Boxplot of Waist Circumference by Sex\n\n\n\n\n\n\n\n\n\nFigure 16. Boxplot of Waist Circumference by Race/Ethnicity Code\n\n\n\n\nFigures 13-16 show base manipulations of the data for waist circumference versus other predictors. Notice that distribution and sex are well balanced, while the scatterplot shows a noticeable decrease as MVPA goes up. Additionally, note the boxplot outlier in Race/Ethnicity code 6 where the mean is much lower than the others. We’ll investigate these trends using MLR, Ridge Regression, Lasso Regression, and Random Forest to assess accuracy and sensitivity across models. Performing MLR with cross-validation, we get results as specified in Table 17.\n\n\n\nTable 17. MLR performance metrics\n\n\nrmse_mean\nrmse_sd\nmae_mean\nmae_sd\nrsq_mean\nrsq_sd\n\n\n\n\n18.14753\n0.3643531\n14.40548\n0.3331936\n-3.454478\n0.0860984\n\n\n\n\n\nThen using the model, we apply it to the test set of data, resulting in Table 18.\n\n\n\nTable 18. Results of MLR model with test data set\n\n\nrmse\nmae\nrsq\n\n\n\n\n17.51308\n14.10237\n-3.552695\n\n\n\n\n\nComparing the two tables, we note that RSQ is negative, which indicates that MLR is either a very poor fit for the data or there is a problem with the model.\nWhen performing Ridge Regression on the same data, we get the results contained in Table 19.\n\n\n\nTable 19. Ridge regression metric performance results\n\n\nrmse\nmae\nrsq\n\n\n\n\n15.74937\n12.35245\n0.0795298\n\n\n\n\n\nComparing these values with previous results from MLR, we note that RMSE is more in line with the CV results, while MAE is in line with both previous values. RSQ is completely different but indicates only a 6.6% fit of the variability of the data.\nPerforming a Lasso Regression, we obtain results shown in Table 20.\n\n\n\nTable 20. Lasso regression metric performance results\n\n\nrmse\nmae\nrsq\n\n\n\n\n15.74135\n12.35013\n0.0804672\n\n\n\n\n\nThe values shown match very closely with the results from the Ridge Regression (which makes sense). Similarly, the comparison to MLR is the same as the comparison from the Ridge Regression. Additionally, we output the list of non-zero coefficients, showing that most are present, which accounts for the minimal differences between Ridge and Lasso.\nFinally, we performed a Random Forest analysis with cross-validation on the data. Results are shown in Table 21.\n\n\n\nTable 21. Random Forest CV results\n\n\nrmse_mean\nrmse_sd\nmae_mean\nmae_sd\nrsq_mean\nrsq_sd\n\n\n\n\n16.08423\n0.3754279\n12.61172\n0.2918258\n0.1250506\n0.0291024\n\n\n\n\n\nCV provides us with values for RMSE and MAE that are slightly lower than previous analysis, however, RSQ is in line with values from Ridge and Lasso regression. The model was then run on the test data and results are summarized in Table 22.\n\n\n\nTable 22. Random Forest results on Test Data\n\n\nrmse\nmae\nrsq\n\n\n\n\n15.57457\n12.24861\n0.0998489\n\n\n\n\n\nThe Random Forest results show that it matches a larger percentage of the variability of the data, up to between 9% and 11%.\nAdditionally, we wanted to know which factors are the most important (significant) in this model. Table 23 shows the importance of factors in descending order. Note the top 4 factors are much higher than the remaining factors.\n\n\n\nTable 23. Top 20 RF important features\n\n\n\nfeature\nimportance\nmethod\n\n\n\n\nage\nage\n162524.312\nRF_importance\n\n\nindfmpir\nindfmpir\n148046.060\nRF_importance\n\n\nmvpa_equiv_minwk\nmvpa_equiv_minwk\n120836.645\nRF_importance\n\n\nrace6\nrace6\n53460.400\nRF_importance\n\n\nsexFemale\nsexFemale\n26812.319\nRF_importance\n\n\nrace4\nrace4\n17368.652\nRF_importance\n\n\nrace3\nrace3\n16477.883\nRF_importance\n\n\neducation4\neducation4\n14504.783\nRF_importance\n\n\neducation5\neducation5\n14127.158\nRF_importance\n\n\neducation3\neducation3\n13516.310\nRF_importance\n\n\neducation2\neducation2\n10019.030\nRF_importance\n\n\nrace7\nrace7\n8351.698\nRF_importance\n\n\nrace2\nrace2\n8161.818\nRF_importance\n\n\n\n\n\nLastly, Figure 17 shows the top predictors across the different methods. Each method favors different predictors, however, there are several commonalities. For example, age and race6 were significant factors in every model. Additional Figure 18 below shows the data in a heat map across models.\n\n\n\n\n\nFigure 17. Top Predictors of Waist Circumference by Modeling Technique\n\n\n\n\n\n\n\n\n\nFigure 18. Cross-Model Consistency by Predictor Rank"
  },
  {
    "objectID": "final_proj.html#conclusions",
    "href": "final_proj.html#conclusions",
    "title": "A Multimodel Analysis of Income, Physical Activity, and Body Composition in U.S. Adults",
    "section": "6 Conclusions",
    "text": "6 Conclusions\nThis study evaluated the relationships between demographic characteristics, physical activity measures, and body composition outcomes using NHANES 2017–2018 data across multiple modeling approaches. For all research questions, the results consistently demonstrated limited explanatory power, with linear, regularized, and tree-based models accounting for only a small proportion of outcome variability. The convergence of weak performance across model types suggests that the absence of meaningful associations is not attributable to model choice alone, but rather reflects limited signal within the available predictors.\nThese findings indicate that commonly used demographic and self-reported physical activity variables may be insufficient to explain individual-level variation in body composition outcomes within a cross-sectional survey framework. The results underscore the importance of rigorous model comparison and cross-validation when evaluating observational health data and demonstrate that null or weak findings can be informative.\n\n6.1 Limitations\nSeveral limitations should be considered when interpreting the findings of this study. First, the analysis relies on cross-sectional data from the NHANES 2017–2018 cycle. As a result, while the statistical models identify associations between demographic factors, physical activity measures, and body composition outcomes, they do not support causal inference. Observed relationships may be influenced by unmeasured confounders or reverse causality that cannot be resolved without additional data.\nSecond, many key variables in NHANES—particularly those related to physical activity behaviors—are self-reported. Self-reported data are subject to recall bias, social desirability bias, and measurement error, which may attenuate or inflate estimated associations. Although NHANES employs standardized survey instruments, these limitations remain inherent to questionnaire-based data collection.\nThird, while NHANES is designed to be nationally representative, this study did not explicitly incorporate survey weights, strata, and primary sampling units into all modeling procedures. As a result, the estimates produced by the regression and machine-learning models may reflect internal associations within the analytic sample rather than population-level estimates. This trade-off was made to prioritize model comparison, cross-validation, and predictive performance, but it limits the generalizability of the findings.\nFourth, the analytical methods used—such as multiple linear regression, ridge and lasso regression, and tree-based models—require assumptions that may not be fully satisfied. Linear models assume linearity, independence, and homoscedasticity, while tree-based methods may be sensitive to sample size and tuning parameters. Although cross-validation was employed to mitigate overfitting, model performance may still vary when applied to different NHANES cycles or external datasets.\nFinally, missing data and exclusion criteria reduced the effective sample size for some research questions. Participants with incomplete demographic, physical activity, or body measurement data were excluded, which may introduce selection bias if excluded individuals differ systematically from those included in the final analytic sample.\n\n\n6.2 Future Work\nFuture research could address these limitations in several important ways. First, extending the analysis to include multiple NHANES cycles would increase sample size, improve statistical power, and allow for examination of temporal consistency in observed relationships. Pooling cycles could also enable stratified analyses across age groups, sex, or racial and ethnic subpopulations.\nSecond, future studies could incorporate survey-weighted regression and machine-learning approaches to better reflect the complex NHANES sampling design and improve population-level inference. Comparing weighted and unweighted model results would provide additional insight into the robustness and generalization of the findings.\nThird, the inclusion of objective physical activity measures, such as accelerometer data available in select NHANES cycles, would help reduce reliance on self-reported behaviors and improve measurement accuracy. Combining objective and self-reported measures may also clarify discrepancies between perceived and actual activity levels.\nAdditional methodological extensions could also include more advanced modeling techniques (not really a focus of this project)."
  },
  {
    "objectID": "final_proj.html#references",
    "href": "final_proj.html#references",
    "title": "A Multimodel Analysis of Income, Physical Activity, and Body Composition in U.S. Adults",
    "section": "7 References",
    "text": "7 References\n[1] “National Health and Nutrition Examination Survey, 2017-2018 Data Documentation, Codebook, and Frequencies, Demographic Variables and Sample Weights (DEMO_J).” Accessed: Nov. 19, 2025. [Online]. Available: https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/DEMO_J.htm\n[2] “National Health and Nutrition Examination Survey, 2017-2018 Data Documentation, Codebook, and Frequencies, Body Measures (BMX_J).” Accessed: Nov. 19, 2025. [Online]. Available: https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/BMX_J.htm\n[3] “National Health and Nutrition Examination Survey, 2017-2018 Data Documentation, Codebook, and Frequencies, Physical Activity (PAQ_J).” Accessed: Nov. 19, 2025. [Online]. Available: https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/PAQ_J.htm\n[4] “National Health and Nutrition Examination Survey, 2017-2018 Data Files, Demographic Variables and Sample Weights (DEMO_J).” Accessed: Nov. 19, 2025. [Online]. Available: https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/DEMO_J.xpt\n[5] “National Health and Nutrition Examination Survey, 2017-2018 Data Files, Body Measures (BMX_J).” Accessed: Nov. 19, 2025. [Online]. Available: https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/BMX_J.xpt\n[6] “National Health and Nutrition Examination Survey, 2017-2018 Data Files, Physical Activity (PAQ_J).” Accessed: Nov. 19, 2025. [Online]. Available: https://wwwn.cdc.gov/Nchs/Data/Nhanes/Public/2017/DataFiles/PAQ_J.xpt\n[7] OpenAI, “ChatGPT,” GPT-5.2, discussions pertaining to data set selection, report structure, and explanation, Dec. 14, 2025.\n[8] R Core Team, R: A Language and Environment for Statistical Computing, version 4.5.1, R Foundation for Statistical Computing, Vienna, Austria, 2025. [Online]. Available: https://www.R-project.org/\n[9] Posit Software, PBC, RStudio: Integrated Development Environment for R, version 2024.09.1, Boston, MA, USA, 2024. [Online]. Available: https://posit.co/products/open-source/rstudio/\n[10] H. Wickham, ggplot2: Elegant Graphics for Data Analysis, version 4.0.0, R package, 2025. [Online]. Available: https://CRAN.R-project.org/package=ggplot2\n[11] H. Wickham, R. François, L. Henry, K. Müller, D. Vaughan, dplyr: A Grammar of Data Manipulation, version 1.1.4, R package, 2023. [Online]. Available: https://CRAN.R-project.org/package=dplyr\n[12] H. Wickham, J. Grolemund, tidyverse: Easily Install and Load the ‘Tidyverse’, version 2.0.0, R package, 2023. [Online]. Available: https://CRAN.R-project.org/package=tidyverse\n[13] C. Sievert, plotly: Create Interactive Web Graphics via ‘plotly.js’, version 4.11.0, R package, 2025. [Online]. Available: https://CRAN.R-project.org/package=plotly\n[14] H. Wickham and E. Miller, haven: Import and Export SPSS, Stata and SAS Files, R package version 2.5.4, 2023. [Online]. Available: https://CRAN.R-project.org/package=haven\n[15] A. Liaw and M. Wiener, randomForest: Breiman and Cutler’s Random Forests for Classification and Regression, R package version 4.7-1.2. [Online]. Available: https://cran.r-project.org/web/packages/randomForest/\n[16] S. Firke, janitor: Simple Tools for Examining and Cleaning Dirty Data, R package version 2.2.0, 2023. [Online]. Available: https://CRAN.R-project.org/package=janitor\n[17] W. Revelle, psych: Procedures for Psychological, Psychometric, and Personality Research, Northwestern University, Evanston, Illinois, R package version 2.4.3, 2024. [Online]. Available: https://CRAN.R-project.org/package=psych\n[18] Y. Xie, knitr: A General-Purpose Package for Dynamic Report Generation in R, R package version 1.45, 2023. [Online]. Available: https://CRAN.R-project.org/package=knitr\n[19] H. Zhu, kableExtra: Construct Complex Table with knitr::kable() + pipe, R package version 1.4.0, 2023. [Online]. Available: https://CRAN.R-project.org/package=kableExtra\n[20] M. Kuhn, caret: Classification and Regression Training, R package version 6.0-94, 2023. [Online]. Available: https://cran.r-project.org/package=caret"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Blake. I’m a retired US Navy officer going back to school to learn something new. Most of my previous experience and learning was focused on Computers and Cybersecurity, so I figured this time around I’d like to focus on different ways to analyze data (I love spreadsheets for some reason…). I’ve been married to my husband Zach for 3 years and we have two dogs, Briggs (Vizsla - 4 y/o) and Maverick (Weimaraner - almost 5 months old)."
  },
  {
    "objectID": "about.html#blake-lafever",
    "href": "about.html#blake-lafever",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Blake. I’m a retired US Navy officer going back to school to learn something new. Most of my previous experience and learning was focused on Computers and Cybersecurity, so I figured this time around I’d like to focus on different ways to analyze data (I love spreadsheets for some reason…). I’ve been married to my husband Zach for 3 years and we have two dogs, Briggs (Vizsla - 4 y/o) and Maverick (Weimaraner - almost 5 months old)."
  },
  {
    "objectID": "about.html#academic-background",
    "href": "about.html#academic-background",
    "title": "About",
    "section": "Academic Background",
    "text": "Academic Background\nBachelor’s of Science in Computer Engineering - Georgia Institute of Technology (2003)\nMasters of Engineering Management - Old Dominion University (2012)\nMaster of Science in Cyber Systems and Operations - Naval Postgraduate School (2015)"
  },
  {
    "objectID": "about.html#professional-background",
    "href": "about.html#professional-background",
    "title": "About",
    "section": "Professional Background",
    "text": "Professional Background\nNuclear Engineer (Submarines) - US Navy (2003-2010)\nSatellite Communications and Computer Networks - US Navy (2010-2023)\nPrincipal Cybersecurity Engineer - Everfox (2023-Present)"
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "During the course of the DAEN degree here at GMU, I will use this page to publish the main projects for each of my different courses."
  },
  {
    "objectID": "portfolio.html#future-class-page",
    "href": "portfolio.html#future-class-page",
    "title": "Portfolio",
    "section": "Future Class Page",
    "text": "Future Class Page\nLinks will go here"
  }
]